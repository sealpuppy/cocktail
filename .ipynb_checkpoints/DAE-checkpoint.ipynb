{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_dir = '/Users/Terry/Documents/Research/test_block/' \n",
    "mix_dir = '/Users/Terry/Documents/Research/test_block/' \n",
    "clean_label_dir = '/Users/Terry/Documents/Research/test_block/' \n",
    "mix_label_dir = '/Users/Terry/Documents/Research/test_block/' \n",
    "\n",
    "# cleanfolder = os.listdir(clean_dir)\n",
    "# cleanfolder.sort()\n",
    "\n",
    "# mixfolder = os.listdir(mix_dir)\n",
    "# mixfolder.sort()\n",
    "\n",
    "# cleanlabelfolder = os.listdir(clean_label_dir)\n",
    "# cleanlabelfolder.sort()\n",
    "\n",
    "# mixlabelfolder = os.listdir(mix_label_dir)\n",
    "# mixlabelfolder.sort()\n",
    "\n",
    "# clean_list = []\n",
    "# mix_list = []\n",
    "# clean_label_list = []\n",
    "# mix_label_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSourceDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir, mix_dir, clean_label_dir, mix_label_dir):\n",
    "        \n",
    "        with open(clean_dir + 'clean0.json') as f:\n",
    "            clean0 = torch.Tensor(json.load(f))\n",
    "        \n",
    "#         with open(clean_label_dir + 'clean_label0.json') as f:\n",
    "#             cleanlabel0 = torch.Tensor(json.load(f))\n",
    "            \n",
    "        self.spec = clean0\n",
    "#         self.label = cleanlabel0\n",
    "            \n",
    "        \n",
    "#         for i in cleanfolder:\n",
    "#             with open(clean_dir + '{}'.format(i)) as f:\n",
    "#                 clean_list.append(torch.Tensor(json.load(f)))\n",
    "\n",
    "#         for i in mixfolder:\n",
    "#             with open(mix_dir + '{}'.format(i)) as f:\n",
    "#                 mix_list.append(torch.Tensor(json.load(f)))\n",
    "                \n",
    "#         for i in cleanlabelfolder:\n",
    "#             with open(clean_label_dir + '{}'.format(i)) as f:\n",
    "#                 clean_label_list.append(torch.Tensor(json.load(f)))\n",
    "\n",
    "#         for i in mixlabelfolder:\n",
    "#             with open(mix_label_dir + '{}'.format(i)) as f:\n",
    "#                 mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "#         cleanblock = torch.cat(clean_list, 0)\n",
    "#         mixblock = torch.cat(mix_list, 0)\n",
    "#         self.spec = torch.cat([cleanblock, mixblock], 0)\n",
    "                \n",
    "#         cleanlabel = torch.cat(clean_label_list, 0)\n",
    "#         mixlabel = torch.cat(mix_label_list, 0)\n",
    "#         self.label = torch.cat([cleanlabel, mixlabel], 0)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset = MSourceDataSet(clean_dir, mix_dir, clean_label_dir, mix_label_dir)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                batch_size = 4,\n",
    "                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "a = trainset.__len__()\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResDAE(\n",
      "  (upward_net1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (upward_net2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (upward_net3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (upward_net4): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (upward_net5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (upward_net6): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (upward_net7): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (downward_net7): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (downward_net6): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (uconv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net5): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (uconv4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net4): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (uconv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net3): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (uconv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net2): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(8, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      "  (downward_net1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(1, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "''' ResBlock '''\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.channels_out > self.channels_in:\n",
    "            x1 = F.relu(self.conv1(x))\n",
    "            x1 =        self.conv2(x1)\n",
    "            x  = self.sizematch(self.channels_in, self.channels_out, x)\n",
    "            return x + x1\n",
    "        elif self.channels_out < self.channels_in:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x1 =       self.conv2(x)\n",
    "            x = x + x1\n",
    "            return x\n",
    "        else:\n",
    "            x1 = F.relu(self.conv1(x))\n",
    "            x1 =        self.conv2(x1)\n",
    "            x = x + x1\n",
    "            return x\n",
    "\n",
    "    def sizematch(self, channels_in, channels_out, x):\n",
    "        zeros = torch.zeros( (x.size()[0], channels_out - channels_in, x.shape[2], x.shape[3]), dtype = torch.float32)\n",
    "        return torch.cat((x, zeros), dim=1)\n",
    "\n",
    "class ResTranspose(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResTranspose, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(2,2), stride=2)\n",
    "        self.deconv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cin = cout\n",
    "        x1 = F.relu(self.deconv1(x))\n",
    "        x1 =        self.deconv2(x1)\n",
    "        x = self.sizematch(x)\n",
    "        return x + x1\n",
    "\n",
    "    def sizematch(self, x):\n",
    "        # expand\n",
    "        x2 = torch.zeros(x.shape[0], self.channels_in, x.shape[2]*2, x.shape[3]*2)\n",
    "\n",
    "        row_x  = torch.zeros(x.shape[0], self.channels_in, x.shape[2], 2*x.shape[3])\n",
    "        row_x[:,:,:,odd(x.shape[3]*2)]   = x\n",
    "        row_x[:,:,:,even(x.shape[3]*2)]  = x\n",
    "        x2[:,:, odd(x.shape[2]*2),:] = row_x\n",
    "        x2[:,:,even(x.shape[2]*2),:] = row_x\n",
    "\n",
    "        return x2\n",
    "\n",
    "\n",
    "def initialize(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "class ResDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResDAE, self).__init__()\n",
    "\n",
    "        # 128x128x1\n",
    "\n",
    "        self.upward_net1 = nn.Sequential(\n",
    "            ResBlock(1, 8),\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 8),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        # 64x64x8\n",
    "\n",
    "        self.upward_net2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 16),\n",
    "            ResBlock(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        # 32x32x16\n",
    "\n",
    "        self.upward_net3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(16, 16),\n",
    "            ResBlock(16, 32),\n",
    "            ResBlock(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # 16x16x32\n",
    "\n",
    "        self.upward_net4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 64),\n",
    "            ResBlock(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # 8x8x64\n",
    "\n",
    "        self.upward_net5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 128),\n",
    "            ResBlock(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # 4x4x128\n",
    "\n",
    "        self.upward_net6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 256),\n",
    "            ResBlock(256, 256),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # 2x2x256\n",
    "\n",
    "        self.upward_net7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(256, 256),\n",
    "            ResBlock(256, 512),\n",
    "            ResBlock(512, 512),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "\n",
    "        # 1x1x512\n",
    "        self.downward_net7 = nn.Sequential(\n",
    "            ResBlock(512, 512),\n",
    "            ResBlock(512, 256),\n",
    "            ResBlock(256, 256),\n",
    "            ResTranspose(256, 256),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # 2x2x256\n",
    "\n",
    "        self.downward_net6 = nn.Sequential(\n",
    "            # 8x8x64\n",
    "            ResBlock(256, 256),\n",
    "            ResBlock(256, 128),\n",
    "            ResBlock(128, 128),\n",
    "            ResTranspose(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # 4x4x128\n",
    "        # cat -> 4x4x256\n",
    "        self.uconv5 = nn.Conv2d(256, 128, kernel_size=(3,3), padding=(1,1))\n",
    "        # 4x4x128\n",
    "        self.downward_net5 = nn.Sequential(\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 64),\n",
    "            ResBlock(64, 64),\n",
    "            ResTranspose(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # 8x8x64\n",
    "        # cat -> 8x8x128\n",
    "        self.uconv4 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n",
    "        # 8x8x64\n",
    "        self.downward_net4 = nn.Sequential(\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 32),\n",
    "            ResBlock(32, 32),\n",
    "            ResTranspose(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # 16x16x32\n",
    "        # cat -> 16x16x64\n",
    "        self.uconv3 = nn.Conv2d(64, 32, kernel_size=(3,3), padding=(1,1))\n",
    "        # 16x16x32\n",
    "        self.downward_net3 = nn.Sequential(\n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 16),\n",
    "            ResBlock(16, 16),\n",
    "            ResTranspose(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        # 32x32x16\n",
    "        # cat -> 32x32x32\n",
    "        self.uconv2 = nn.Conv2d(32, 16, kernel_size=(3,3), padding=(1,1))\n",
    "        # 32x32x16\n",
    "        self.downward_net2 = nn.Sequential(\n",
    "            ResBlock(16, 16),\n",
    "            ResBlock(16, 8),\n",
    "            ResBlock(8, 8),\n",
    "            ResTranspose(8, 8),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        # 64x64x8\n",
    "        self.downward_net1 = nn.Sequential(\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 4),\n",
    "            ResBlock(4, 1),\n",
    "            ResBlock(1, 1),\n",
    "            ResTranspose(1, 1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "        # 128x128x1\n",
    "\n",
    "\n",
    "    def upward(self, x, a7=None, a6=None, a5=None, a4=None, a3=None, a2=None):\n",
    "        x = x.view(bs, 1, 1025, 16)\n",
    "        # 1x128x128\n",
    "\n",
    "        x = self.upward_net1(x)\n",
    "        # 8x64x64\n",
    "\n",
    "        x = self.upward_net2(x)\n",
    "        if a2 is not None: x = x * a2\n",
    "        self.x2 = x\n",
    "        # 16x32x32\n",
    "\n",
    "        x = self.upward_net3(x)\n",
    "        if a3 is not None: x = x * a3\n",
    "        self.x3 = x\n",
    "        # 32x16x16\n",
    "\n",
    "        x = self.upward_net4(x)\n",
    "        if a4 is not None: x = x * a4\n",
    "        self.x4 = x\n",
    "        # 64x8x8\n",
    "\n",
    "        x = self.upward_net5(x)\n",
    "        if a5 is not None: x = x * a5\n",
    "        self.x5 = x\n",
    "        # 128x4x4\n",
    "\n",
    "        x = self.upward_net6(x)\n",
    "        if a6 is not None: x = x * a6\n",
    "        # 256x2x2\n",
    "\n",
    "        x = self.upward_net7(x)\n",
    "        if a7 is not None: x = x * a7\n",
    "        # 512x1x1\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def downward(self, y, shortcut=True):\n",
    "\n",
    "        # 512x1x1\n",
    "        y = self.downward_net7(y)\n",
    "\n",
    "        # 256x2x2\n",
    "        y = self.downward_net6(y)\n",
    "\n",
    "        # 128x4x4\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x5), 1)\n",
    "            y = F.relu(self.uconv5(y))\n",
    "        y = self.downward_net5(y)\n",
    "\n",
    "        # 64x8x8\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x4), 1)\n",
    "            y = F.relu(self.uconv4(y))\n",
    "        y = self.downward_net4(y)\n",
    "        \n",
    "        # 32x16x16\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x3), 1)\n",
    "            y = F.relu(self.uconv3(y))\n",
    "        y = self.downward_net3(y)\n",
    "\n",
    "        # 16x32x32\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x2), 1)\n",
    "            y = F.relu(self.uconv2(y))\n",
    "        y = self.downward_net2(y)\n",
    "\n",
    "        # 8x64x64\n",
    "        y = self.downward_net1(y)\n",
    "        \n",
    "        # 1x128x128\n",
    "\n",
    "        return y\n",
    "\n",
    "model = ResDAE()\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr = 0.005\n",
    "mom = 0.005\n",
    "bs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(size_average = True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = mom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 這部分還沒加進去\n",
    "\n",
    "def odd(w):\n",
    "    return list(np.arange(1, w, step=2, dtype='long'))\n",
    "\n",
    "def even(w):\n",
    "    return list(np.arange(0, w, step=2, dtype='long'))\n",
    "\n",
    "def white(x):\n",
    "    fw, tw = x.shape[1], x.shape[2]\n",
    "\n",
    "    first = F.relu(torch.normal(mean=torch.zeros(fw, tw), std=torch.ones(fw, tw)) ) * 0.05\n",
    "    second_seed = F.relu(torch.normal(mean=torch.zeros(fw//2, tw//2), std=torch.ones(fw//2, tw//2))) * 0.03\n",
    "    second = torch.zeros(fw, tw)\n",
    "\n",
    "    row_x  = torch.zeros(int(fw//2), tw)\n",
    "    # row_x = torch.zeros(int(fw/2), tw)\n",
    "\n",
    "    row_x[:, odd(tw)]  = second_seed\n",
    "    row_x[:, even(tw)] = second_seed\n",
    "\n",
    "    second[odd(fw), :]  = row_x\n",
    "    second[even(fw), :] = row_x\n",
    "\n",
    "    return second + first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1025, 16])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.normal received an invalid combination of arguments - got (std=torch.FloatTensor, mean=torch.FloatTensor, ), but expected one of:\n * (torch.FloatTensor std)\n * (torch.FloatTensor means)\n * (float mean, torch.FloatTensor std)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mmean=torch.FloatTensor\u001b[0m, \u001b[32;1mstd=torch.FloatTensor\u001b[0m, )\n * (torch.FloatTensor means, float std)\n      didn't match because some of the keywords were incorrect: mean\n * (torch.FloatTensor means, torch.FloatTensor std)\n      didn't match because some of the keywords were incorrect: mean\n * (torch.Generator generator, torch.FloatTensor std)\n      didn't match because some of the keywords were incorrect: mean\n * (torch.Generator generator, torch.FloatTensor means)\n      didn't match because some of the keywords were incorrect: std, mean\n * (torch.Generator generator, float mean, torch.FloatTensor std)\n * (torch.Generator generator, torch.FloatTensor means, float std)\n * (torch.Generator generator, torch.FloatTensor means, torch.FloatTensor std)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e7358cdb3c71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwhite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8c2b0d9bbc1d>\u001b[0m in \u001b[0;36mwhite\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0msecond_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msecond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.normal received an invalid combination of arguments - got (std=torch.FloatTensor, mean=torch.FloatTensor, ), but expected one of:\n * (torch.FloatTensor std)\n * (torch.FloatTensor means)\n * (float mean, torch.FloatTensor std)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mmean=torch.FloatTensor\u001b[0m, \u001b[32;1mstd=torch.FloatTensor\u001b[0m, )\n * (torch.FloatTensor means, float std)\n      didn't match because some of the keywords were incorrect: mean\n * (torch.FloatTensor means, torch.FloatTensor std)\n      didn't match because some of the keywords were incorrect: mean\n * (torch.Generator generator, torch.FloatTensor std)\n      didn't match because some of the keywords were incorrect: mean\n * (torch.Generator generator, torch.FloatTensor means)\n      didn't match because some of the keywords were incorrect: std, mean\n * (torch.Generator generator, float mean, torch.FloatTensor std)\n * (torch.Generator generator, torch.FloatTensor means, float std)\n * (torch.Generator generator, torch.FloatTensor means, torch.FloatTensor std)\n"
     ]
    }
   ],
   "source": [
    "loss_record = []\n",
    "every_loss = []\n",
    "epoch_loss = []\n",
    "\n",
    "model.train()\n",
    "for epo in range(epoch):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = data\n",
    "        inputs = Variable(inputs)\n",
    "        print (inputs.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        top = model.upward(inputs + white(inputs))\n",
    "        outputs = model.downward(top, shortcut = True)\n",
    "\n",
    "        \n",
    "        loss = criterion(inputs, outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_record.append(loss.item())\n",
    "        every_loss.append(loss.item())\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print ('[%d, %5d] loss: %.3f' % (epo, i, loss.item()))\n",
    "        \n",
    "    epoch_loss.append(np.mean(every_loss))\n",
    "    every_loss = []\n",
    "        \n",
    "        \n",
    "# # forward\n",
    "# top = model.upward(source + white(source))\n",
    "# recover = model.downward(top).view(end - block_pos, 128, 128)\n",
    "\n",
    "# # loss-bp\n",
    "# loss = criterion(optouts, recover)\n",
    "# loss.backward()\n",
    "\n",
    "\n",
    "# # step\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
