{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "import pytorch_ssim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "bs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd(w):\n",
    "    return list(np.arange(1, w, step=2, dtype='long'))\n",
    "\n",
    "def even(w):\n",
    "    return list(np.arange(0, w, step=2, dtype='long'))\n",
    "\n",
    "def white(x):\n",
    "    fw, tw = x.shape[1], x.shape[2]\n",
    "\n",
    "    first = F.relu(torch.normal(mean=torch.zeros(fw, tw), std=torch.ones(fw, tw)) ) * 0.05\n",
    "    second_seed = F.relu(torch.normal(mean=torch.zeros(fw//2, tw//2), std=torch.ones(fw//2, tw//2))) * 0.03\n",
    "    second = torch.zeros(fw, tw)\n",
    "\n",
    "    row_x  = torch.zeros(int(fw//2), tw)\n",
    "    # row_x = torch.zeros(int(fw/2), tw)\n",
    "\n",
    "    row_x[:, odd(tw)]  = second_seed\n",
    "    row_x[:, even(tw)] = second_seed\n",
    "\n",
    "    second[odd(fw), :]  = row_x\n",
    "    second[even(fw), :] = row_x\n",
    "\n",
    "    return second + first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#        path\n",
    "#=============================================\n",
    "\n",
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = root_dir + 'clean/' \n",
    "# mix_dir = root_dir + 'mix/' \n",
    "# clean_label_dir = root_dir + 'clean_labels/' \n",
    "# mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "# mixfolder = os.listdir(mix_dir)\n",
    "# mixfolder.sort()\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "# mix_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSourceDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir):\n",
    "        \n",
    "\n",
    "        # Overfitting single block\n",
    "        with open(clean_dir + 'clean11.json') as f:\n",
    "            clean_list.append(torch.Tensor(json.load(f)))\n",
    "\n",
    "            \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "#         mixblock = torch.cat(mix_list, 0)\n",
    "        self.spec = cleanblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        return spec\n",
    "    \n",
    "#=============================================\n",
    "#        Define Dataloader\n",
    "#=============================================\n",
    "\n",
    "trainset = MSourceDataSet(clean_dir)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = True)\n",
    "#=============================================\n",
    "#        Define Dataloader\n",
    "#=============================================\n",
    "\n",
    "trainset = MSourceDataSet(clean_dir)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.local/lib/python3.6/site-packages/torch/serialization.py:400: UserWarning: Couldn't retrieve source code for container of type ResBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n",
      "/home/tk/.local/lib/python3.6/site-packages/torch/serialization.py:400: UserWarning: Couldn't retrieve source code for container of type ResTranspose. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResDAE(\n",
      "  (upward_net1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upward_net2): Sequential(\n",
      "    (0): Conv2d(8, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upward_net3): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upward_net4): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upward_net5): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upward_net6): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upward_net7): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (downward_net7): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (downward_net6): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(128, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (uconv5): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net5): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(64, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (uconv4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net4): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (uconv3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net3): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(16, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (uconv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (downward_net2): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResTranspose(\n",
      "      (deconv1): ConvTranspose2d(8, 8, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (downward_net1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv1): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#=============================================\n",
    "#        Model\n",
    "#=============================================\n",
    "\n",
    "''' ResBlock '''\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.channels_out > self.channels_in:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x  = self.sizematch(self.channels_in, self.channels_out, x)\n",
    "            return F.relu(x + x1, inplace = True)\n",
    "        elif self.channels_out < self.channels_in:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x1 =       self.conv2(x)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "        else:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "\n",
    "    def sizematch(self, channels_in, channels_out, x):\n",
    "        zeros = torch.zeros( (x.size()[0], channels_out - channels_in, x.shape[2], x.shape[3]), dtype = torch.float32)\n",
    "        return torch.cat((x, zeros), dim=1)\n",
    "\n",
    "class ResTranspose(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResTranspose, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(2,2), stride=2)\n",
    "        self.deconv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cin = cout\n",
    "        x1 = F.relu(self.deconv1(x), inplace = True)\n",
    "        x1 =        self.deconv2(x1)\n",
    "        x = self.sizematch(x)\n",
    "        return F.relu(x + x1, inplace = True)\n",
    "\n",
    "    def sizematch(self, x):\n",
    "        # expand\n",
    "        x2 = torch.zeros(x.shape[0], self.channels_in, x.shape[2]*2, x.shape[3]*2)\n",
    "\n",
    "        row_x  = torch.zeros(x.shape[0], self.channels_in, x.shape[2], 2*x.shape[3])\n",
    "        row_x[:,:,:,odd(x.shape[3]*2)]   = x\n",
    "        row_x[:,:,:,even(x.shape[3]*2)]  = x\n",
    "        x2[:,:, odd(x.shape[2]*2),:] = row_x\n",
    "        x2[:,:,even(x.shape[2]*2),:] = row_x\n",
    "\n",
    "        return x2\n",
    "\n",
    "\n",
    "def initialize(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = ResDAE()\n",
    "model = torch.load(root_dir + 'recover/SSIM-CONV/DAE_SSIM.pkl')\n",
    "print (model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pytorch_ssim' has no attribute 'SSIM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-b190c99bfbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import pytorch_ssim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_ssim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSIM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, momentum = mom)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pytorch_ssim' has no attribute 'SSIM'"
     ]
    }
   ],
   "source": [
    "\n",
    "#=============================================\n",
    "#        Optimizer\n",
    "#=============================================\n",
    "\n",
    "#import pytorch_ssim\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) #, momentum = mom)\n",
    "\n",
    "#=============================================\n",
    "#        Loss Record\n",
    "#=============================================\n",
    "\n",
    "loss_record = []\n",
    "# every_loss = []\n",
    "# epoch_loss = []\n",
    "\n",
    "#=============================================\n",
    "#        test\n",
    "#=============================================\n",
    "\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "\n",
    "model.eval()\n",
    "for epo in range(epoch):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = data\n",
    "        inputs = Variable(inputs)\n",
    "        top = model.upward(inputs + white(inputs))\n",
    "        \n",
    "        outputs = model.downward(top, shortcut = True)\n",
    "        inputs = inputs.view(bs, 1, 256, 128)\n",
    "        outputs = outputs.view(bs, 1, 256, 128)\n",
    "        #with open ( root_dir + 'recover/L1loss_FC/recover_pic_epo_' + str(epo), 'w') as f:\n",
    "        #    json.dump(outputs.tolist(), f)\n",
    "        \n",
    "        loss = - criterion(outputs, inputs)\n",
    "        ssim_value = - loss.data.item()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            inn = inputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/test/\" + str(epo) + \"_\" + str(i) + \".png\", inn)\n",
    "            \n",
    "            out = outputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/test/\" + str(epo) + \"_\" + str(i) + \"_re.png\", out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#=============================================\n",
    "#        Hyperparameters\n",
    "#=============================================\n",
    "\n",
    "epoch = 2\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "bs = 16\n",
    "\n",
    "#======================================\n",
    "clean_dir = '/home/tk/Documents/clean/' \n",
    "clean_label_dir = '/home/tk/Documents/clean_labels/' \n",
    "#========================================\n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "cleanlabelfolder = os.listdir(clean_label_dir)\n",
    "cleanlabelfolder.sort()\n",
    "\n",
    "clean_list = []\n",
    "clean_label_list = []\n",
    "\n",
    "#========================================\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir, clean_label_dir):\n",
    "                \n",
    "\n",
    "        for i in cleanfolder:\n",
    "            with open(clean_dir + '{}'.format(i)) as f:\n",
    "                clean_list.append(torch.Tensor(json.load(f)))\n",
    "                \n",
    "        for i in cleanlabelfolder:\n",
    "            with open(clean_label_dir + '{}'.format(i)) as f:\n",
    "                clean_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "        self.spec = torch.cat([cleanblock], 0)\n",
    "                \n",
    "        cleanlabel = torch.cat(clean_label_list, 0)\n",
    "        self.label = torch.cat([cleanlabel], 0)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        label = self.label[index]\n",
    "        return spec, label\n",
    "\n",
    "    \n",
    "#=================================================    \n",
    "#           Dataloader \n",
    "#=================================================\n",
    "featureset = featureDataSet(clean_dir, clean_label_dir)\n",
    "trainloader = torch.utils.data.DataLoader(dataset = featureset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = True)\n",
    "\n",
    "#=================================================    \n",
    "#           model \n",
    "#=================================================\n",
    "class featureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(featureNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(256 * 128, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#model = featureNet()\n",
    "model = torch.load('/home/tk/Documents/FeatureNet.pkl')\n",
    "print (model)\n",
    "\n",
    "#============================================\n",
    "#              optimizer\n",
    "#============================================\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = mom)\n",
    "\n",
    "#============================================\n",
    "#              training\n",
    "#============================================\n",
    "\n",
    "loss_record = []\n",
    "every_loss = []\n",
    "epoch_loss = []\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(70):\n",
    "    \n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_record.append(loss.item())\n",
    "        every_loss.append(loss.item())\n",
    "        print ('[%d, %5d] loss: %.3f' % (epoch, i, loss.item()))\n",
    "        \n",
    "    epoch_loss.append(np.mean(every_loss))\n",
    "    every_loss = []\n",
    "\n",
    "            \n",
    "            \n",
    "torch.save(model, '/home/tk/Documents/FeatureNet.pkl')\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(loss_record)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(epoch_loss)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('epoch_loss')\n",
    "plt.savefig('epoch_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureNet(\n",
      "  (fc1): Linear(in_features=32768, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.local/lib/python3.6/site-packages/torch/serialization.py:400: UserWarning: Couldn't retrieve source code for container of type featureNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('/home/tk/Documents/FeatureNet.pkl')\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "j = open('/home/tk/Documents/mix/mix0.json').read()\n",
    "a = json.loads(j)\n",
    "mix = np.array(a)\n",
    "print (mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2)\n"
     ]
    }
   ],
   "source": [
    "j = open('/home/tk/Documents/mix_labels/mix_label0.json').read()\n",
    "a = json.loads(j)\n",
    "mix_label = np.array(a)\n",
    "print(mix_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 7],\n",
       "       [4, 6],\n",
       "       [4, 3],\n",
       "       [9, 1],\n",
       "       [8, 9],\n",
       "       [8, 1],\n",
       "       [7, 4],\n",
       "       [9, 7],\n",
       "       [0, 8],\n",
       "       [4, 1],\n",
       "       [8, 3],\n",
       "       [8, 1],\n",
       "       [6, 4],\n",
       "       [7, 0],\n",
       "       [7, 0],\n",
       "       [8, 6],\n",
       "       [0, 6],\n",
       "       [8, 0],\n",
       "       [6, 0],\n",
       "       [0, 9]])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "25\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "index1 = (i * 11) + mix_label[i][0]\n",
    "index2 = (i * 11) + mix_label[i][1]\n",
    "print (index1)\n",
    "print (index2)\n",
    "mix_index = i * 11 + 10\n",
    "print (mix_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_label[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 1.00000000e+00\n",
      "  1.00000000e+00 1.02176239e-01]\n",
      " [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 1.00000000e+00\n",
      "  1.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 1.00000000e+00 1.00000000e+00 ... 5.19889498e-01\n",
      "  5.42978210e-01 1.00000000e+00]\n",
      " ...\n",
      " [1.44138199e-03 2.03927551e-03 2.31814442e-03 ... 4.62637811e-04\n",
      "  7.89675421e-04 8.49219901e-04]\n",
      " [1.07287918e-03 1.37008745e-03 1.50386281e-03 ... 1.53794700e-04\n",
      "  3.07196968e-04 6.21216520e-04]\n",
      " [2.42032224e-04 1.49996426e-04 1.15045391e-04 ... 2.18073619e-03\n",
      "  2.96849246e-03 4.27753112e-03]]\n"
     ]
    }
   ],
   "source": [
    "cv2.imwrite('/home/tk/Desktop/a.png', mix[index1] * 255)\n",
    "print (mix[index1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.05284773e-05 3.98410597e-05 3.98345154e-05 ... 8.90631516e-05\n",
      "  1.15941706e-04 1.17861566e-04]\n",
      " [1.88307903e-05 1.80878629e-05 1.81974799e-05 ... 5.65265754e-05\n",
      "  9.99650600e-05 1.53485486e-04]\n",
      " [8.65778473e-07 7.13725829e-07 4.45877835e-07 ... 1.63591404e-06\n",
      "  1.28119965e-05 5.98908866e-05]\n",
      " ...\n",
      " [5.29056491e-09 5.91402978e-09 6.67889369e-09 ... 3.67486706e-08\n",
      "  2.50067570e-08 3.39107073e-09]\n",
      " [2.43333927e-09 4.11830265e-09 6.64502577e-09 ... 3.35489202e-07\n",
      "  2.86786076e-07 2.75487550e-07]\n",
      " [8.44342994e-10 1.02317247e-09 2.83612406e-09 ... 7.25919010e-07\n",
      "  3.56982595e-07 8.09511382e-08]]\n"
     ]
    }
   ],
   "source": [
    "cv2.imwrite('/home/tk/Desktop/b.png', mix[index2] * 255)\n",
    "print (mix[index2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.06613262e-05 2.42840316e-03 1.95877882e-03 ... 2.03071674e-03\n",
      "  5.45274516e-03 6.77584875e-03]\n",
      " [3.33156840e-02 4.44002733e-02 4.48987597e-02 ... 1.16924211e-02\n",
      "  1.33293181e-02 1.24881586e-02]\n",
      " [7.92597808e-02 2.95037199e-02 1.41161245e-02 ... 1.24883816e-02\n",
      "  1.28239769e-02 1.10428574e-02]\n",
      " ...\n",
      " [1.51715146e-06 2.67569451e-06 9.76789590e-06 ... 1.14631439e-04\n",
      "  1.21727369e-04 1.10448973e-04]\n",
      " [9.46525271e-06 5.23733453e-06 1.60264138e-06 ... 7.87736115e-05\n",
      "  5.14321400e-05 3.06227908e-05]\n",
      " [8.51934868e-06 6.39732463e-06 5.68628732e-06 ... 4.30707022e-05\n",
      "  4.04523268e-05 3.49109294e-05]]\n"
     ]
    }
   ],
   "source": [
    "print (mix[mix_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (mix[index1] + mix[index2])\n",
    "c = np.clip(c, np.min(c), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.06613262e-05 2.42840316e-03 1.95877882e-03 ... 2.03071674e-03\n",
      "  5.45274516e-03 6.77584875e-03]\n",
      " [3.33156840e-02 4.44002733e-02 4.48987597e-02 ... 1.16924211e-02\n",
      "  1.33293181e-02 1.24881586e-02]\n",
      " [7.92597808e-02 2.95037199e-02 1.41161245e-02 ... 1.24883816e-02\n",
      "  1.28239769e-02 1.10428574e-02]\n",
      " ...\n",
      " [1.51715146e-06 2.67569451e-06 9.76789590e-06 ... 1.14631439e-04\n",
      "  1.21727369e-04 1.10448973e-04]\n",
      " [9.46525271e-06 5.23733453e-06 1.60264138e-06 ... 7.87736115e-05\n",
      "  5.14321400e-05 3.06227908e-05]\n",
      " [8.51934868e-06 6.39732463e-06 5.68628732e-06 ... 4.30707022e-05\n",
      "  4.04523268e-05 3.49109294e-05]]\n"
     ]
    }
   ],
   "source": [
    "cv2.imwrite('/home/tk/Desktop/mix.png', c * 255)\n",
    "print (mix[mix_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4]\n"
     ]
    }
   ],
   "source": [
    "print (mix_label[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * 11 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print (mix_label[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('/home/tk/Documents/clean/clean0.json') as f:\n",
    "    b = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-6fac886a1cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/tk/Documents/clean/clean0.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             raise TypeError('the JSON object must be str, bytes or bytearray, '\n\u001b[0;32m--> 348\u001b[0;31m                             'not {!r}'.format(s.__class__.__name__))\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not 'list'"
     ]
    }
   ],
   "source": [
    "j = open('/home/tk/Documents/clean/clean0.json').read()\n",
    "b = json.loads(b)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tk/Documents/mix_labels/mix_label0.json') as f:\n",
    "    a = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400\n"
     ]
    }
   ],
   "source": [
    "print (len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros((10))\n",
    "print (z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/tk/Documents/'\n",
    "clean_dir = root_dir + 'clean/' \n",
    "\n",
    "clean_list = []\n",
    "clean_label_list = []\n",
    "mix_list = []\n",
    "mix_label_list = []\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "c[i*2: (i+1)*2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[1],[1],[2],[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, c6lean_dir):\n",
    "\n",
    "        with open(clean_dir + 'clean3.json') as f:\n",
    "            clean_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(root_dir + \"clean_labels/clean_label3.json\") as f:\n",
    "            clean_label_list.append(torch.Tensor(json.load(f)))\n",
    "\n",
    "        with open(root_dir + 'mix/mix0.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "            \n",
    "        with open(root_dir + 'mix_labels/mix_label0.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "            \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        self.spec = cleanblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        return spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "for h in range(0, 200, 20):\n",
    "    print (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0.5\n",
    "multiplication = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n",
      "checked\n",
      "birdstudybook_0_9.wav\n"
     ]
    }
   ],
   "source": [
    "spec_name = os.listdir(root_dir + \"slice_pointsec/\")\n",
    "if len(spec_name) == (10/length)* 10 * multiplication: \n",
    "    print ((10/length)* 10 * multiplication)\n",
    "    print (\"checked\")\n",
    "spec_name.sort()\n",
    "print (spec_name[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = data_process.gen_spectrogram(root_dir + 'slice_pointcec/' + spec_name[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0 + i, int(10/length) * 10 * multiplication + i, int(10/length) * multiplication):\n",
    "    spec = data_process.gen_spectrogram(root_dir + 'slice_pointsec/' + spec_name[j])\n",
    "    small_pcs.append(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "for j in range(0, 200, 20):\n",
    "    print (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'json' has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-c17ef76086aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'json' has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "json.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 5, 5])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3,4,5,6,7]\n",
    "np.clip(l, min(l), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = root_dir + 'clean/' \n",
    "mix_dir = root_dir + 'mix/' \n",
    "clean_label_dir = root_dir + 'clean_labels/' \n",
    "mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "mixfolder = os.listdir(mix_dir)\n",
    "mixfolder.sort()\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "mix_list = []\n",
    "mix_label_list = []\n",
    "feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, mix_dir, mix_label_dir):           \n",
    "        \n",
    "        with open(mix_dir + 'mix3.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(mix_label_dir + 'mix_label3.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        mixlabel = torch.cat(mix_label_list, 0)\n",
    "        \n",
    "        self.mix_spec = mixblock\n",
    "        self.mix_label = mixlabel\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.mix_spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        mix_spec = self.mix_spec[index]\n",
    "        mix_label = self.mix_label[index]\n",
    "        return mix_spec, mix_label\n",
    "\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir):\n",
    "        \n",
    "\n",
    "        with open(clean_dir + 'clean3.json') as f:\n",
    "            feature_list.append(torch.Tensor(json.load(f)))      \n",
    "        \n",
    "        featureblock = torch.cat(feature_list, 0)\n",
    "        self.featurespec = featureblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.featurespec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        featurespec = self.featurespec[index]\n",
    "        return featurespec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset = mixDataSet(mix_dir, mix_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspec, mlabel = mixset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlabel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = featureDataSet(clean_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixloader = torch.utils.data.DataLoader(dataset = MSourceDataSet(clean_dir),\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-f80bfe26ee3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmixset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmix_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmix_label_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m mixloader = torch.utils.data.DataLoader(dataset = MSourceDataSet(clean_dir),\n\u001b[1;32m      5\u001b[0m                                                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-367-b6e5e449d200>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clean_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mfeature_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mfeatureblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeaturespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureblock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "mixset = mixDataSet(mix_dir, mix_label_dir)\n",
    "featureset = featureDataSet(clean_dir)\n",
    "\n",
    "mixloader = torch.utils.data.DataLoader(dataset = MSourceDataSet(clean_dir),\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = False)\n",
    "\n",
    "featureloader = torch.utils.data.DataLoader(dataset = featureset,\n",
    "                                           batch_size = bs,\n",
    "                                           shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_spec, mix_label = mixset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.1211e-02, 9.6732e-02, 1.1441e-01,  ..., 2.1542e-03, 3.2162e-03,\n",
       "         4.1116e-03],\n",
       "        [6.6107e-02, 9.0755e-02, 1.0918e-01,  ..., 8.4901e-03, 5.8412e-03,\n",
       "         5.1425e-03],\n",
       "        [6.5641e-03, 5.3293e-03, 7.2872e-03,  ..., 4.8888e-03, 4.3776e-03,\n",
       "         3.3129e-03],\n",
       "        ...,\n",
       "        [1.7836e-02, 2.2978e-02, 2.5137e-02,  ..., 6.1808e-05, 1.0718e-04,\n",
       "         1.8373e-04],\n",
       "        [1.9758e-02, 1.9567e-02, 1.5679e-02,  ..., 1.3179e-04, 2.1113e-04,\n",
       "         3.0304e-04],\n",
       "        [8.6527e-03, 6.5997e-03, 4.4745e-03,  ..., 1.0750e-04, 1.7081e-04,\n",
       "         2.7934e-04]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-6f82c18574ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'clean3.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             raise TypeError('the JSON object must be str, bytes or bytearray, '\n\u001b[0;32m--> 348\u001b[0;31m                             'not {!r}'.format(s.__class__.__name__))\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not 'list'"
     ]
    }
   ],
   "source": [
    "j = open(clean_dir + 'clean3.json').read()\n",
    "b = json.loads(b)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "j = open('/home/tk/Documents/clean/clean0.json').read()\n",
    "a = json.loads(j)\n",
    "mix = np.array(a)\n",
    "print (mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "import pytorch_ssim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as npu\n",
    "import gc\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "bs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd(w):\n",
    "    return list(np.arange(1, w, step=2, dtype='long'))\n",
    "\n",
    "def even(w):\n",
    "    return list(np.arange(0, w, step=2, dtype='long'))\n",
    "\n",
    "def white(x):\n",
    "    fw, tw = x.shape[1], x.shape[2]\n",
    "\n",
    "    first = F.relu(torch.normal(mean=torch.zeros(fw, tw), std=torch.ones(fw, tw)) ) * 0.05\n",
    "    second_seed = F.relu(torch.normal(mean=torch.zeros(fw//2, tw//2), std=torch.ones(fw//2, tw//2))) * 0.03\n",
    "    second = torch.zeros(fw, tw)\n",
    "\n",
    "    row_x  = torch.zeros(int(fw//2), tw)\n",
    "    # row_x = torch.zeros(int(fw/2), tw)\n",
    "\n",
    "    row_x[:, odd(tw)]  = second_seed\n",
    "    row_x[:, even(tw)] = second_seed\n",
    "\n",
    "    second[odd(fw), :]  = row_x\n",
    "    second[even(fw), :] = row_x\n",
    "\n",
    "    return second + first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = root_dir + 'clean/' \n",
    "mix_dir = root_dir + 'mix/' \n",
    "clean_label_dir = root_dir + 'clean_labels/' \n",
    "mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "mixfolder = os.listdir(mix_dir)\n",
    "mixfolder.sort()\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "mix_list = []\n",
    "mix_label_list = []\n",
    "feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, mix_dir, mix_label_dir):           \n",
    "        \n",
    "        with open(mix_dir + 'mix3.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(mix_label_dir + 'mix_label3.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        mixlabel = torch.cat(mix_label_list, 0)\n",
    "        \n",
    "        self.mix_spec = mixblock\n",
    "        self.mix_label = mixlabel\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.mix_spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        mix_spec = self.spec[index]\n",
    "        mix_label = self.mix_label[index]\n",
    "        return mix_spec, mix_label\n",
    "\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir):\n",
    "        \n",
    "\n",
    "        with open(clean_dir + 'clean3.json') as f:\n",
    "            feature_list.append(torch.Tensor(json.load(f)))      \n",
    "        \n",
    "        featureblock = torch.cat(clean_list, 0)\n",
    "        self.featurespec = featureblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.featurespec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        featurespec = self.featurespec[index]\n",
    "        return featurespec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'mix_label_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-368-92c0fa25192b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmixset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmix_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeatureset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatureDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m mixloader = torch.utils.data.DataLoader(dataset = mixset,\n\u001b[1;32m      5\u001b[0m                                                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'mix_label_dir'"
     ]
    }
   ],
   "source": [
    "mixset = mixDataSet(mix_dir)\n",
    "featureset = featureDataset(clean_dir)\n",
    "\n",
    "mixloader = torch.utils.data.DataLoader(dataset = mixset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = False)\n",
    "\n",
    "featureloader = torch.utils.data.DataLoader(dataset = featureset,\n",
    "                                           batch_size = bs,\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featureNet'''\n",
    "class featureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1025*16, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1025*16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "#feature_model = featureNet()\n",
    "feature_model = torch.load(root_dir + 'FeatureNet.pkl')\n",
    "    \n",
    "'''ANet'''\n",
    "class ANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANet, self).__init__()\n",
    "\n",
    "        self.linear7 = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear6 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear5 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear4 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(bs, 1, 256)\n",
    "\n",
    "        a7 = self.linear7(x).view(bs, 512, 1, 1)\n",
    "        a6 = self.linear6(x).view(bs, 256, 1, 1)\n",
    "        a5 = self.linear5(x).view(bs, 128, 1, 1)\n",
    "        a4 = self.linear4(x).view(bs, 64, 1, 1)\n",
    "        a3 = self.linear3(x).view(bs, 32, 1, 1)\n",
    "        a2 = self.linear2(x).view(bs, 16, 1, 1)\n",
    "\n",
    "return a7, a6, a5, a4, a3, a2\n",
    "\n",
    "A_model = ANet()\n",
    "\n",
    "\n",
    "''' ResBlock '''\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.channels_out > self.channels_in:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x  = self.sizematch(self.channels_in, self.channels_out, x)\n",
    "            return F.relu(x + x1, inplace = True)\n",
    "        elif self.channels_out < self.channels_in:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x1 =       self.conv2(x)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "        else:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "\n",
    "    def sizematch(self, channels_in, channels_out, x):\n",
    "        zeros = torch.zeros( (x.size()[0], channels_out - channels_in, x.shape[2], x.shape[3]), dtype = torch.float32)\n",
    "        return torch.cat((x, zeros), dim=1)\n",
    "\n",
    "class ResTranspose(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResTranspose, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(2,2), stride=2)\n",
    "        self.deconv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cin = cout\n",
    "        x1 = F.relu(self.deconv1(x), inplace = True)\n",
    "        x1 =        self.deconv2(x1)\n",
    "        x = self.sizematch(x)\n",
    "        return F.relu(x + x1, inplace = True)\n",
    "\n",
    "    def sizematch(self, x):\n",
    "        # expand\n",
    "        x2 = torch.zeros(x.shape[0], self.channels_in, x.shape[2]*2, x.shape[3]*2)\n",
    "\n",
    "        row_x  = torch.zeros(x.shape[0], self.channels_in, x.shape[2], 2*x.shape[3])\n",
    "        row_x[:,:,:,odd(x.shape[3]*2)]   = x\n",
    "        row_x[:,:,:,even(x.shape[3]*2)]  = x\n",
    "        x2[:,:, odd(x.shape[2]*2),:] = row_x\n",
    "        x2[:,:,even(x.shape[2]*2),:] = row_x\n",
    "\n",
    "        return x2\n",
    "\n",
    "\n",
    "def initialize(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "class ResDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResDAE, self).__init__()\n",
    "\n",
    "        # 128x128x1\n",
    "\n",
    "        self.upward_net1 = nn.Sequential(\n",
    "            ResBlock(1, 1),\n",
    "            ResBlock(1, 8),\n",
    "            ResBlock(8, 8),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        # 64x64x8\n",
    "\n",
    "        self.upward_net2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 16),\n",
    "            ResBlock(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        # 32x32x16\n",
    "\n",
    "        self.upward_net3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(16, 16),\n",
    "            ResBlock(16, 32),\n",
    "            ResBlock(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # 16x16x32\n",
    "\n",
    "        self.upward_net4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 64),\n",
    "            ResBlock(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # 8x8x64\n",
    "\n",
    "        self.upward_net5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 128),\n",
    "            ResBlock(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # 4x4x128\n",
    "\n",
    "        self.upward_net6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 256),\n",
    "            ResBlock(256, 256),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # 2x2x256\n",
    "\n",
    "        self.upward_net7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(256, 256),\n",
    "            ResBlock(256, 512),\n",
    "            ResBlock(512, 512),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "\n",
    "        # 1x1x512\n",
    "        self.downward_net7 = nn.Sequential(\n",
    "            ResBlock(512, 512),\n",
    "            ResBlock(512, 256),\n",
    "            ResBlock(256, 256),\n",
    "            ResTranspose(256, 256),\n",
    "#            nn.ConvTranspose2d(256, 256, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # 2x2x256\n",
    "\n",
    "        self.downward_net6 = nn.Sequential(\n",
    "            # 8x8x64\n",
    "            ResBlock(256, 256),\n",
    "            ResBlock(256, 128),\n",
    "            ResBlock(128, 128),\n",
    "            ResTranspose(128, 128),\n",
    "#            nn.ConvTranspose2d(128, 128, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # 4x4x128\n",
    "        # cat -> 4x4x256\n",
    "        self.uconv5 = nn.Conv2d(256, 128, kernel_size=(3,3), padding=(1,1))\n",
    "        # 4x4x128\n",
    "        self.downward_net5 = nn.Sequential(\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 64),\n",
    "            ResBlock(64, 64),\n",
    "            ResTranspose(64, 64),\n",
    "#            nn.ConvTranspose2d(64, 64, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # 8x8x64\n",
    "        # cat -> 8x8x128\n",
    "        self.uconv4 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n",
    "        # 8x8x64\n",
    "        self.downward_net4 = nn.Sequential(\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 32),\n",
    "            ResBlock(32, 32),\n",
    "            ResTranspose(32, 32),\n",
    "#            nn.ConvTranspose2d(32, 32, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # 16x16x32\n",
    "        # cat -> 16x16x64\n",
    "        self.uconv3 = nn.Conv2d(64, 32, kernel_size=(3,3), padding=(1,1))\n",
    "        # 16x16x32\n",
    "        self.downward_net3 = nn.Sequential(\n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 16),\n",
    "            ResBlock(16, 16),\n",
    "            ResTranspose(16, 16),\n",
    "#            nn.ConvTranspose2d(16, 16, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        # 32x32x16\n",
    "        # cat -> 32x32x32\n",
    "        self.uconv2 = nn.Conv2d(32, 16, kernel_size=(3,3), padding=(1,1))\n",
    "        # 32x32x16\n",
    "        self.downward_net2 = nn.Sequential(\n",
    "            ResBlock(16, 16),\n",
    "            ResBlock(16, 8),\n",
    "            ResBlock(8, 8),\n",
    "            ResTranspose(8, 8),\n",
    "#            nn.ConvTranspose2d(8, 8, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        # 64x64x8\n",
    "        self.downward_net1 = nn.Sequential(\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 1),\n",
    "            ResBlock(1, 1),\n",
    "            ResBlock(1, 1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "        # 128x128x1\n",
    "        \n",
    "        self.apply(initialize)\n",
    "\n",
    "\n",
    "    def upward(self, x, a7= True, a6= True, a5= True, a4= True, a3= True, a2= True):\n",
    "        x = x.view(bs, 1, 256, 128)\n",
    "\n",
    "        # 1x256x128\n",
    "#        print (\"initial\", x.shape)\n",
    "\n",
    "        x = self.upward_net1(x)\n",
    "#        print (\"after conv1\", x.shape)\n",
    "\n",
    "\n",
    "        # 8x128x64\n",
    "        x = self.upward_net2(x)\n",
    "        if a2 is not None: x = x * a2\n",
    "        self.x2 = x\n",
    "#        print (\"after conv2\", x.shape)\n",
    "\n",
    "        # 16x64x32\n",
    "        x = self.upward_net3(x)\n",
    "        if a3 is not None: x = x * a3\n",
    "        self.x3 = x\n",
    "#        print (\"after conv3\", x.shape)\n",
    "\n",
    "        # 32x32x16\n",
    "        x = self.upward_net4(x)\n",
    "        if a4 is not None: x = x * a4\n",
    "        self.x4 = x\n",
    "#        print (\"after conv4\", x.shape)\n",
    "\n",
    "        # 64x16x8\n",
    "        x = self.upward_net5(x)\n",
    "        if a5 is not None: x = x * a5\n",
    "        self.x5 = x\n",
    "#        print (\"after conv5\", x.shape)\n",
    "\n",
    "        \n",
    "        # 128x8x4\n",
    "        x = self.upward_net6(x)\n",
    "        if a6 is not None: x = x * a6\n",
    "#        print (\"after conv6\", x.shape)\n",
    "\n",
    "        # 256x4x2\n",
    "        x = self.upward_net7(x)\n",
    "        if a7 is not None: x = x * a7\n",
    "#        print (\"after conv7\", x.shape)\n",
    "\n",
    "        # 512x2x1\n",
    "        return x\n",
    "\n",
    "\n",
    "    def downward(self, y, shortcut= True):\n",
    "#        print (\"begin to downward, y.shape = \", y.shape)\n",
    "        # 512x2x1\n",
    "        y = self.downward_net7(y)\n",
    "#        print (\"after down7\", y.shape)\n",
    "\n",
    "        # 256x4x2\n",
    "        y = self.downward_net6(y)\n",
    "#        print (\"after down6\", y.shape)\n",
    "\n",
    "        # 128x8x4\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x5), 1)\n",
    "            y = F.relu(self.uconv5(y))\n",
    "        y = self.downward_net5(y)\n",
    "#        print (\"after down5\", y.shape)\n",
    "\n",
    "        # 64x16x8\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x4), 1)\n",
    "            y = F.relu(self.uconv4(y))\n",
    "        y = self.downward_net4(y)\n",
    "#        print (\"after down4\", y.shape)\n",
    "\n",
    "        # 32x32x16\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x3), 1)\n",
    "            y = F.relu(self.uconv3(y))\n",
    "        y = self.downward_net3(y)\n",
    "#        print (\"after down3\", y.shape)\n",
    "\n",
    "        # 16x64x32\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x2), 1)\n",
    "            y = F.relu(self.uconv2(y))\n",
    "        y = self.downward_net2(y)\n",
    "#        print (\"after down2\", y.shape)\n",
    "\n",
    "        # 8x128x64\n",
    "        y = self.downward_net1(y)\n",
    "#        print (\"after down1\", y.shape)\n",
    " \n",
    "        # 1x256x128\n",
    "        return y\n",
    "\n",
    "# Res_model = ResDAE()\n",
    "Res_model = torch.load(root_dir + 'recover/SSIM-CONV/DAE_SSIM.pkl')\n",
    "# print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_ssim\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) #, momentum = mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epo in range(epoch):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = data\n",
    "        inputs = Variable(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        top = model.upward(inputs + white(inputs))\n",
    "        \n",
    "        outputs = model.downward(top, shortcut = True)\n",
    "        inputs = inputs.view(bs, 1, 256, 128)\n",
    "        outputs = outputs.view(bs, 1, 256, 128)\n",
    "        #with open ( root_dir + 'recover/L1loss_FC/recover_pic_epo_' + str(epo), 'w') as f:\n",
    "        #    json.dump(outputs.tolist(), f)\n",
    "        \n",
    "        loss = - criterion(outputs, inputs)\n",
    "        ssim_value = - loss.data.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_record.append(loss.item())\n",
    "        plt.figure(figsize = (20, 10))\n",
    "        plt.plot(loss_record)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('loss')\n",
    "        plt.savefig(root_dir + 'recover/SSIM-CONV/DAE_loss.png')\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            inn = inputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/\" + str(epo) + \"_\" + str(i) + \".png\", inn)\n",
    "            \n",
    "            out = outputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/\" + str(epo) + \"_\" + str(i) + \"_re.png\", out)\n",
    "            \n",
    "            \n",
    "#        every_loss.append(loss.item())\n",
    "#        del inputs, data\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print ('[%d, %5d] loss: %.3f' % (epo, i, loss.item()))\n",
    "#            print ('[%d, %5d] ssim: %.3f' % (epo, i, ssim_value))\n",
    "   \n",
    "    gc.collect()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "#    epoch_loss.append(np.mean(every_loss))\n",
    "#    every_loss = []\n",
    "    \n",
    "\n",
    "\n",
    "#        plt.figure(figsize = (20, 10))\n",
    "#        plt.plot(epoch_loss)\n",
    "#        plt.xlabel('epocs')\n",
    "#        plt.ylabel('epoch_loss')\n",
    "#        plt.savefig(root_dir + 'DAE_epoch_loss')\n",
    "\n",
    "    \n",
    "#=============================================\n",
    "#        Save Model & Loss\n",
    "#=============================================\n",
    "\n",
    "torch.save(model, root_dir + 'recover/SSIM-CONV/DAE_SSIM.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
