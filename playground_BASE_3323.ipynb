{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model = torch.load(root_dir + 'FeatureNet.pkl')\n",
    "print (feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "import pytorch_ssim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "bs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd(w):\n",
    "    return list(np.arange(1, w, step=2, dtype='long'))\n",
    "\n",
    "def even(w):\n",
    "    return list(np.arange(0, w, step=2, dtype='long'))\n",
    "\n",
    "def white(x):\n",
    "    fw, tw = x.shape[1], x.shape[2]\n",
    "\n",
    "    first = F.relu(torch.normal(mean=torch.zeros(fw, tw), std=torch.ones(fw, tw)) ) * 0.05\n",
    "    second_seed = F.relu(torch.normal(mean=torch.zeros(fw//2, tw//2), std=torch.ones(fw//2, tw//2))) * 0.03\n",
    "    second = torch.zeros(fw, tw)\n",
    "\n",
    "    row_x  = torch.zeros(int(fw//2), tw)\n",
    "    # row_x = torch.zeros(int(fw/2), tw)\n",
    "\n",
    "    row_x[:, odd(tw)]  = second_seed\n",
    "    row_x[:, even(tw)] = second_seed\n",
    "\n",
    "    second[odd(fw), :]  = row_x\n",
    "    second[even(fw), :] = row_x\n",
    "\n",
    "    return second + first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#        path\n",
    "#=============================================\n",
    "\n",
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = root_dir + 'clean/' \n",
    "# mix_dir = root_dir + 'mix/' \n",
    "# clean_label_dir = root_dir + 'clean_labels/' \n",
    "# mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "# mixfolder = os.listdir(mix_dir)\n",
    "# mixfolder.sort()\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "# mix_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSourceDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir):\n",
    "        \n",
    "\n",
    "        # Overfitting single block\n",
    "        with open(clean_dir + 'clean11.json') as f:\n",
    "            clean_list.append(torch.Tensor(json.load(f)))\n",
    "\n",
    "            \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "#         mixblock = torch.cat(mix_list, 0)\n",
    "        self.spec = cleanblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        return spec\n",
    "    \n",
    "#=============================================\n",
    "#        Define Dataloader\n",
    "#=============================================\n",
    "\n",
    "trainset = MSourceDataSet(clean_dir)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = True)\n",
    "#=============================================\n",
    "#        Define Dataloader\n",
    "#=============================================\n",
    "\n",
    "trainset = MSourceDataSet(clean_dir)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset = trainset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#        Model\n",
    "#=============================================\n",
    "\n",
    "''' ResBlock '''\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.channels_out > self.channels_in:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x  = self.sizematch(self.channels_in, self.channels_out, x)\n",
    "            return F.relu(x + x1, inplace = True)\n",
    "        elif self.channels_out < self.channels_in:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x1 =       self.conv2(x)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "        else:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "\n",
    "    def sizematch(self, channels_in, channels_out, x):\n",
    "        zeros = torch.zeros( (x.size()[0], channels_out - channels_in, x.shape[2], x.shape[3]), dtype = torch.float32)\n",
    "        return torch.cat((x, zeros), dim=1)\n",
    "\n",
    "class ResTranspose(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResTranspose, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(2,2), stride=2)\n",
    "        self.deconv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cin = cout\n",
    "        x1 = F.relu(self.deconv1(x), inplace = True)\n",
    "        x1 =        self.deconv2(x1)\n",
    "        x = self.sizematch(x)\n",
    "        return F.relu(x + x1, inplace = True)\n",
    "\n",
    "    def sizematch(self, x):\n",
    "        # expand\n",
    "        x2 = torch.zeros(x.shape[0], self.channels_in, x.shape[2]*2, x.shape[3]*2)\n",
    "\n",
    "        row_x  = torch.zeros(x.shape[0], self.channels_in, x.shape[2], 2*x.shape[3])\n",
    "        row_x[:,:,:,odd(x.shape[3]*2)]   = x\n",
    "        row_x[:,:,:,even(x.shape[3]*2)]  = x\n",
    "        x2[:,:, odd(x.shape[2]*2),:] = row_x\n",
    "        x2[:,:,even(x.shape[2]*2),:] = row_x\n",
    "\n",
    "        return x2\n",
    "\n",
    "\n",
    "def initialize(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = ResDAE()\n",
    "model = torch.load(root_dir + 'recover/SSIM-CONV/DAE_SSIM.pkl')\n",
    "print (model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#=============================================\n",
    "#        Optimizer\n",
    "#=============================================\n",
    "\n",
    "#import pytorch_ssim\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) #, momentum = mom)\n",
    "\n",
    "#=============================================\n",
    "#        Loss Record\n",
    "#=============================================\n",
    "\n",
    "loss_record = []\n",
    "# every_loss = []\n",
    "# epoch_loss = []\n",
    "\n",
    "#=============================================\n",
    "#        test\n",
    "#=============================================\n",
    "\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "\n",
    "model.eval()\n",
    "for epo in range(epoch):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = data\n",
    "        inputs = Variable(inputs)\n",
    "        top = model.upward(inputs + white(inputs))\n",
    "        \n",
    "        outputs = model.downward(top, shortcut = True)\n",
    "        inputs = inputs.view(bs, 1, 256, 128)\n",
    "        outputs = outputs.view(bs, 1, 256, 128)\n",
    "        #with open ( root_dir + 'recover/L1loss_FC/recover_pic_epo_' + str(epo), 'w') as f:\n",
    "        #    json.dump(outputs.tolist(), f)\n",
    "        \n",
    "        loss = - criterion(outputs, inputs)\n",
    "        ssim_value = - loss.data.item()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            inn = inputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/test/\" + str(epo) + \"_\" + str(i) + \".png\", inn)\n",
    "            \n",
    "            out = outputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/test/\" + str(epo) + \"_\" + str(i) + \"_re.png\", out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#=============================================\n",
    "#        Hyperparameters\n",
    "#=============================================\n",
    "\n",
    "epoch = 2\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "bs = 16\n",
    "\n",
    "#======================================\n",
    "clean_dir = '/home/tk/Documents/clean/' \n",
    "clean_label_dir = '/home/tk/Documents/clean_labels/' \n",
    "#========================================\n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "cleanlabelfolder = os.listdir(clean_label_dir)\n",
    "cleanlabelfolder.sort()\n",
    "\n",
    "clean_list = []\n",
    "clean_label_list = []\n",
    "\n",
    "#========================================\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir, clean_label_dir):\n",
    "                \n",
    "\n",
    "        for i in cleanfolder:\n",
    "            with open(clean_dir + '{}'.format(i)) as f:\n",
    "                clean_list.append(torch.Tensor(json.load(f)))\n",
    "                \n",
    "        for i in cleanlabelfolder:\n",
    "            with open(clean_label_dir + '{}'.format(i)) as f:\n",
    "                clean_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "        self.spec = torch.cat([cleanblock], 0)\n",
    "                \n",
    "        cleanlabel = torch.cat(clean_label_list, 0)\n",
    "        self.label = torch.cat([cleanlabel], 0)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        label = self.label[index]\n",
    "        return spec, label\n",
    "\n",
    "    \n",
    "#=================================================    \n",
    "#           Dataloader \n",
    "#=================================================\n",
    "featureset = featureDataSet(clean_dir, clean_label_dir)\n",
    "trainloader = torch.utils.data.DataLoader(dataset = featureset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = True)\n",
    "\n",
    "#=================================================    \n",
    "#           model \n",
    "#=================================================\n",
    "class featureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(featureNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(256 * 128, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 256*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#model = featureNet()\n",
    "model = torch.load('/home/tk/Documents/FeatureNet.pkl')\n",
    "print (model)\n",
    "\n",
    "#============================================\n",
    "#              optimizer\n",
    "#============================================\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = mom)\n",
    "\n",
    "#============================================\n",
    "#              training\n",
    "#============================================\n",
    "\n",
    "loss_record = []\n",
    "every_loss = []\n",
    "epoch_loss = []\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(70):\n",
    "    \n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_record.append(loss.item())\n",
    "        every_loss.append(loss.item())\n",
    "        print ('[%d, %5d] loss: %.3f' % (epoch, i, loss.item()))\n",
    "        \n",
    "    epoch_loss.append(np.mean(every_loss))\n",
    "    every_loss = []\n",
    "\n",
    "            \n",
    "            \n",
    "torch.save(model, '/home/tk/Documents/FeatureNet.pkl')\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(loss_record)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(epoch_loss)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('epoch_loss')\n",
    "plt.savefig('epoch_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/tk/Documents/FeatureNet.pkl')\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('/home/tk/Documents/mix/mix0.json').read()\n",
    "a = json.loads(j)\n",
    "mix = np.array(a)\n",
    "print (mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = open('/home/tk/Documents/mix_labels/mix_label0.json').read()\n",
    "a = json.loads(j)\n",
    "mix_label = np.array(a)\n",
    "print(mix_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "index1 = (i * 11) + mix_label[i][0]\n",
    "index2 = (i * 11) + mix_label[i][1]\n",
    "print (index1)\n",
    "print (index2)\n",
    "mix_index = i * 11 + 10\n",
    "print (mix_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "index1 = (i * 11) + mix_label[i][0]\n",
    "index2 = (i * 11) + mix_label[i][1]\n",
    "print (index1)\n",
    "print (index2)\n",
    "mix_index = i * 11 + 10\n",
    "print (mix_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_label[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('/home/tk/Desktop/a.png', mix[index1] * 255)\n",
    "print (mix[index1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('/home/tk/Desktop/b.png', mix[index2] * 255)\n",
    "print (mix[index2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mix[mix_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = (mix[index1] + mix[index2])\n",
    "c = np.clip(c, np.min(c), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imwrite('/home/tk/Desktop/mix.png', c * 255)\n",
    "print (mix[mix_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mix_label[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * 11 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mix_label[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tk/Documents/clean/clean0.json') as f:\n",
    "    b = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "j = open('/home/tk/Documents/clean/clean0.json').read()\n",
    "b = json.loads(b)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/tk/Documents/mix_labels/mix_label0.json') as f:\n",
    "    a = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros((10))\n",
    "print (z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/tk/Documents/'\n",
    "clean_dir = root_dir + 'clean/' \n",
    "\n",
    "clean_list = []\n",
    "clean_label_list = []\n",
    "mix_list = []\n",
    "mix_label_list = []\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "c[i*2: (i+1)*2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[1],[1],[2],[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, c6lean_dir):\n",
    "\n",
    "        with open(clean_dir + 'clean3.json') as f:\n",
    "            clean_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(root_dir + \"clean_labels/clean_label3.json\") as f:\n",
    "            clean_label_list.append(torch.Tensor(json.load(f)))\n",
    "\n",
    "        with open(root_dir + 'mix/mix0.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "            \n",
    "        with open(root_dir + 'mix_labels/mix_label0.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "            \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        self.spec = cleanblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        return spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in range(0, 200, 20):\n",
    "    print (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0.5\n",
    "multiplication = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_name = os.listdir(root_dir + \"slice_pointsec/\")\n",
    "if len(spec_name) == (10/length)* 10 * multiplication: \n",
    "    print ((10/length)* 10 * multiplication)\n",
    "    print (\"checked\")\n",
    "spec_name.sort()\n",
    "print (spec_name[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = data_process.gen_spectrogram(root_dir + 'slice_pointcec/' + spec_name[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0 + i, int(10/length) * 10 * multiplication + i, int(10/length) * multiplication):\n",
    "    spec = data_process.gen_spectrogram(root_dir + 'slice_pointsec/' + spec_name[j])\n",
    "    small_pcs.append(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, 200, 20):\n",
    "    print (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3,4,5,6,7]\n",
    "np.clip(l, min(l), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = root_dir + 'clean/' \n",
    "mix_dir = root_dir + 'mix/' \n",
    "clean_label_dir = root_dir + 'clean_labels/' \n",
    "mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "mixfolder = os.listdir(mix_dir)\n",
    "mixfolder.sort()\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "mix_list = []\n",
    "mix_label_list = []\n",
    "feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, mix_dir, mix_label_dir):           \n",
    "        \n",
    "        with open(mix_dir + 'mix3.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(mix_label_dir + 'mix_label3.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        mixlabel = torch.cat(mix_label_list, 0)\n",
    "        \n",
    "        self.mix_spec = mixblock\n",
    "        self.mix_label = mixlabel\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.mix_spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        mix_spec = self.mix_spec[index]\n",
    "        mix_label = self.mix_label[index]\n",
    "        return mix_spec, mix_label\n",
    "\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir):\n",
    "        \n",
    "\n",
    "        with open(clean_dir + 'clean3.json') as f:\n",
    "            feature_list.append(torch.Tensor(json.load(f)))      \n",
    "        \n",
    "        featureblock = torch.cat(feature_list, 0)\n",
    "        self.featurespec = featureblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.featurespec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        featurespec = self.featurespec[index]\n",
    "        return featurespec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset = mixDataSet(mix_dir, mix_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mspec, mlabel = mixset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlabel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = featureDataSet(clean_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixloader = torch.utils.data.DataLoader(dataset = MSourceDataSet(clean_dir),\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset = mixDataSet(mix_dir, mix_label_dir)\n",
    "featureset = featureDataSet(clean_dir)\n",
    "\n",
    "mixloader = torch.utils.data.DataLoader(dataset = MSourceDataSet(clean_dir),\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = False)\n",
    "\n",
    "featureloader = torch.utils.data.DataLoader(dataset = featureset,\n",
    "                                           batch_size = bs,\n",
    "                                           shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_spec, mix_label = mixset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open(clean_dir + 'clean3.json').read()\n",
    "b = json.loads(b)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('/home/tk/Documents/clean/clean0.json').read()\n",
    "a = json.loads(j)\n",
    "mix = np.array(a)\n",
    "print (mix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "import pytorch_ssim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as npu\n",
    "import gc\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2\n",
    "lr = 0.001\n",
    "mom = 0.9\n",
    "bs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd(w):\n",
    "    return list(np.arange(1, w, step=2, dtype='long'))\n",
    "\n",
    "def even(w):\n",
    "    return list(np.arange(0, w, step=2, dtype='long'))\n",
    "\n",
    "def white(x):\n",
    "    fw, tw = x.shape[1], x.shape[2]\n",
    "\n",
    "    first = F.relu(torch.normal(mean=torch.zeros(fw, tw), std=torch.ones(fw, tw)) ) * 0.05\n",
    "    second_seed = F.relu(torch.normal(mean=torch.zeros(fw//2, tw//2), std=torch.ones(fw//2, tw//2))) * 0.03\n",
    "    second = torch.zeros(fw, tw)\n",
    "\n",
    "    row_x  = torch.zeros(int(fw//2), tw)\n",
    "    # row_x = torch.zeros(int(fw/2), tw)\n",
    "\n",
    "    row_x[:, odd(tw)]  = second_seed\n",
    "    row_x[:, even(tw)] = second_seed\n",
    "\n",
    "    second[odd(fw), :]  = row_x\n",
    "    second[even(fw), :] = row_x\n",
    "\n",
    "    return second + first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = root_dir + 'clean/' \n",
    "mix_dir = root_dir + 'mix/' \n",
    "clean_label_dir = root_dir + 'clean_labels/' \n",
    "mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "mixfolder = os.listdir(mix_dir)\n",
    "mixfolder.sort()\n",
    "\n",
    "\n",
    "clean_list = []\n",
    "mix_list = []\n",
    "mix_label_list = []\n",
    "feature_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, mix_dir, mix_label_dir):           \n",
    "        \n",
    "        with open(mix_dir + 'mix3.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(mix_label_dir + 'mix_label3.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        mixlabel = torch.cat(mix_label_list, 0)\n",
    "        \n",
    "        self.mix_spec = mixblock\n",
    "        self.mix_label = mixlabel\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.mix_spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        mix_spec = self.spec[index]\n",
    "        mix_label = self.mix_label[index]\n",
    "        return mix_spec, mix_label\n",
    "\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir):\n",
    "        \n",
    "\n",
    "        with open(clean_dir + 'clean3.json') as f:\n",
    "            feature_list.append(torch.Tensor(json.load(f)))      \n",
    "        \n",
    "        featureblock = torch.cat(clean_list, 0)\n",
    "        self.featurespec = featureblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.featurespec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        featurespec = self.featurespec[index]\n",
    "        return featurespec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset = mixDataSet(mix_dir)\n",
    "featureset = featureDataset(clean_dir)\n",
    "\n",
    "mixloader = torch.utils.data.DataLoader(dataset = mixset,\n",
    "                                                batch_size = bs,\n",
    "                                                shuffle = False)\n",
    "\n",
    "featureloader = torch.utils.data.DataLoader(dataset = featureset,\n",
    "                                           batch_size = bs,\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''featureNet'''\n",
    "class featureNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1025*16, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1025*16)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#        x = torch.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "#feature_model = featureNet()\n",
    "feature_model = torch.load(root_dir + 'FeatureNet.pkl')\n",
    "    \n",
    "'''ANet'''\n",
    "class ANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANet, self).__init__()\n",
    "\n",
    "        self.linear7 = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear6 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear5 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear4 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(256, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(bs, 1, 256)\n",
    "\n",
    "        a7 = self.linear7(x).view(bs, 512, 1, 1)\n",
    "        a6 = self.linear6(x).view(bs, 256, 1, 1)\n",
    "        a5 = self.linear5(x).view(bs, 128, 1, 1)\n",
    "        a4 = self.linear4(x).view(bs, 64, 1, 1)\n",
    "        a3 = self.linear3(x).view(bs, 32, 1, 1)\n",
    "        a2 = self.linear2(x).view(bs, 16, 1, 1)\n",
    "\n",
    "return a7, a6, a5, a4, a3, a2\n",
    "\n",
    "A_model = ANet()\n",
    "\n",
    "\n",
    "''' ResBlock '''\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResBlock, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.channels_out > self.channels_in:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x  = self.sizematch(self.channels_in, self.channels_out, x)\n",
    "            return F.relu(x + x1, inplace = True)\n",
    "        elif self.channels_out < self.channels_in:\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x1 =       self.conv2(x)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "        else:\n",
    "            x1 = F.relu(self.conv1(x), inplace = True)\n",
    "            x1 =        self.conv2(x1)\n",
    "            x = x + x1\n",
    "            return F.relu(x, inplace = True)\n",
    "\n",
    "    def sizematch(self, channels_in, channels_out, x):\n",
    "        zeros = torch.zeros( (x.size()[0], channels_out - channels_in, x.shape[2], x.shape[3]), dtype = torch.float32)\n",
    "        return torch.cat((x, zeros), dim=1)\n",
    "\n",
    "class ResTranspose(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super(ResTranspose, self).__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=channels_in, out_channels=channels_out, kernel_size=(2,2), stride=2)\n",
    "        self.deconv2 = nn.Conv2d(in_channels=channels_out, out_channels=channels_out, kernel_size=(3,3), padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cin = cout\n",
    "        x1 = F.relu(self.deconv1(x), inplace = True)\n",
    "        x1 =        self.deconv2(x1)\n",
    "        x = self.sizematch(x)\n",
    "        return F.relu(x + x1, inplace = True)\n",
    "\n",
    "    def sizematch(self, x):\n",
    "        # expand\n",
    "        x2 = torch.zeros(x.shape[0], self.channels_in, x.shape[2]*2, x.shape[3]*2)\n",
    "\n",
    "        row_x  = torch.zeros(x.shape[0], self.channels_in, x.shape[2], 2*x.shape[3])\n",
    "        row_x[:,:,:,odd(x.shape[3]*2)]   = x\n",
    "        row_x[:,:,:,even(x.shape[3]*2)]  = x\n",
    "        x2[:,:, odd(x.shape[2]*2),:] = row_x\n",
    "        x2[:,:,even(x.shape[2]*2),:] = row_x\n",
    "\n",
    "        return x2\n",
    "\n",
    "\n",
    "def initialize(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "\n",
    "\n",
    "\n",
    "class ResDAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResDAE, self).__init__()\n",
    "\n",
    "        # 128x128x1\n",
    "\n",
    "        self.upward_net1 = nn.Sequential(\n",
    "            ResBlock(1, 1),\n",
    "            ResBlock(1, 8),\n",
    "            ResBlock(8, 8),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        # 64x64x8\n",
    "\n",
    "        self.upward_net2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 16),\n",
    "            ResBlock(16, 16),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        # 32x32x16\n",
    "\n",
    "        self.upward_net3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(16, 16),\n",
    "            ResBlock(16, 32),\n",
    "            ResBlock(32, 32),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # 16x16x32\n",
    "\n",
    "        self.upward_net4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 64),\n",
    "            ResBlock(64, 64),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # 8x8x64\n",
    "\n",
    "        self.upward_net5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 128),\n",
    "            ResBlock(128, 128),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # 4x4x128\n",
    "\n",
    "        self.upward_net6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 256),\n",
    "            ResBlock(256, 256),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # 2x2x256\n",
    "\n",
    "        self.upward_net7 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=(2,2), stride=2),\n",
    "            nn.ReLU(),\n",
    "            ResBlock(256, 256),\n",
    "            ResBlock(256, 512),\n",
    "            ResBlock(512, 512),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "\n",
    "        # 1x1x512\n",
    "        self.downward_net7 = nn.Sequential(\n",
    "            ResBlock(512, 512),\n",
    "            ResBlock(512, 256),\n",
    "            ResBlock(256, 256),\n",
    "            ResTranspose(256, 256),\n",
    "#            nn.ConvTranspose2d(256, 256, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "\n",
    "        # 2x2x256\n",
    "\n",
    "        self.downward_net6 = nn.Sequential(\n",
    "            # 8x8x64\n",
    "            ResBlock(256, 256),\n",
    "            ResBlock(256, 128),\n",
    "            ResBlock(128, 128),\n",
    "            ResTranspose(128, 128),\n",
    "#            nn.ConvTranspose2d(128, 128, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # 4x4x128\n",
    "        # cat -> 4x4x256\n",
    "        self.uconv5 = nn.Conv2d(256, 128, kernel_size=(3,3), padding=(1,1))\n",
    "        # 4x4x128\n",
    "        self.downward_net5 = nn.Sequential(\n",
    "            ResBlock(128, 128),\n",
    "            ResBlock(128, 64),\n",
    "            ResBlock(64, 64),\n",
    "            ResTranspose(64, 64),\n",
    "#            nn.ConvTranspose2d(64, 64, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "\n",
    "        # 8x8x64\n",
    "        # cat -> 8x8x128\n",
    "        self.uconv4 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=(1,1))\n",
    "        # 8x8x64\n",
    "        self.downward_net4 = nn.Sequential(\n",
    "            ResBlock(64, 64),\n",
    "            ResBlock(64, 32),\n",
    "            ResBlock(32, 32),\n",
    "            ResTranspose(32, 32),\n",
    "#            nn.ConvTranspose2d(32, 32, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "        )\n",
    "\n",
    "        # 16x16x32\n",
    "        # cat -> 16x16x64\n",
    "        self.uconv3 = nn.Conv2d(64, 32, kernel_size=(3,3), padding=(1,1))\n",
    "        # 16x16x32\n",
    "        self.downward_net3 = nn.Sequential(\n",
    "            ResBlock(32, 32),\n",
    "            ResBlock(32, 16),\n",
    "            ResBlock(16, 16),\n",
    "            ResTranspose(16, 16),\n",
    "#            nn.ConvTranspose2d(16, 16, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(16),\n",
    "        )\n",
    "\n",
    "        # 32x32x16\n",
    "        # cat -> 32x32x32\n",
    "        self.uconv2 = nn.Conv2d(32, 16, kernel_size=(3,3), padding=(1,1))\n",
    "        # 32x32x16\n",
    "        self.downward_net2 = nn.Sequential(\n",
    "            ResBlock(16, 16),\n",
    "            ResBlock(16, 8),\n",
    "            ResBlock(8, 8),\n",
    "            ResTranspose(8, 8),\n",
    "#            nn.ConvTranspose2d(8, 8, kernel_size = (2,2), stride = 2),\n",
    "            nn.BatchNorm2d(8),\n",
    "        )\n",
    "\n",
    "        # 64x64x8\n",
    "        self.downward_net1 = nn.Sequential(\n",
    "            ResBlock(8, 8),\n",
    "            ResBlock(8, 1),\n",
    "            ResBlock(1, 1),\n",
    "            ResBlock(1, 1),\n",
    "            nn.BatchNorm2d(1),\n",
    "        )\n",
    "\n",
    "        # 128x128x1\n",
    "        \n",
    "        self.apply(initialize)\n",
    "\n",
    "\n",
    "    def upward(self, x, a7= True, a6= True, a5= True, a4= True, a3= True, a2= True):\n",
    "        x = x.view(bs, 1, 256, 128)\n",
    "\n",
    "        # 1x256x128\n",
    "#        print (\"initial\", x.shape)\n",
    "\n",
    "        x = self.upward_net1(x)\n",
    "#        print (\"after conv1\", x.shape)\n",
    "\n",
    "\n",
    "        # 8x128x64\n",
    "        x = self.upward_net2(x)\n",
    "        if a2 is not None: x = x * a2\n",
    "        self.x2 = x\n",
    "#        print (\"after conv2\", x.shape)\n",
    "\n",
    "        # 16x64x32\n",
    "        x = self.upward_net3(x)\n",
    "        if a3 is not None: x = x * a3\n",
    "        self.x3 = x\n",
    "#        print (\"after conv3\", x.shape)\n",
    "\n",
    "        # 32x32x16\n",
    "        x = self.upward_net4(x)\n",
    "        if a4 is not None: x = x * a4\n",
    "        self.x4 = x\n",
    "#        print (\"after conv4\", x.shape)\n",
    "\n",
    "        # 64x16x8\n",
    "        x = self.upward_net5(x)\n",
    "        if a5 is not None: x = x * a5\n",
    "        self.x5 = x\n",
    "#        print (\"after conv5\", x.shape)\n",
    "\n",
    "        \n",
    "        # 128x8x4\n",
    "        x = self.upward_net6(x)\n",
    "        if a6 is not None: x = x * a6\n",
    "#        print (\"after conv6\", x.shape)\n",
    "\n",
    "        # 256x4x2\n",
    "        x = self.upward_net7(x)\n",
    "        if a7 is not None: x = x * a7\n",
    "#        print (\"after conv7\", x.shape)\n",
    "\n",
    "        # 512x2x1\n",
    "        return x\n",
    "\n",
    "\n",
    "    def downward(self, y, shortcut= True):\n",
    "#        print (\"begin to downward, y.shape = \", y.shape)\n",
    "        # 512x2x1\n",
    "        y = self.downward_net7(y)\n",
    "#        print (\"after down7\", y.shape)\n",
    "\n",
    "        # 256x4x2\n",
    "        y = self.downward_net6(y)\n",
    "#        print (\"after down6\", y.shape)\n",
    "\n",
    "        # 128x8x4\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x5), 1)\n",
    "            y = F.relu(self.uconv5(y))\n",
    "        y = self.downward_net5(y)\n",
    "#        print (\"after down5\", y.shape)\n",
    "\n",
    "        # 64x16x8\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x4), 1)\n",
    "            y = F.relu(self.uconv4(y))\n",
    "        y = self.downward_net4(y)\n",
    "#        print (\"after down4\", y.shape)\n",
    "\n",
    "        # 32x32x16\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x3), 1)\n",
    "            y = F.relu(self.uconv3(y))\n",
    "        y = self.downward_net3(y)\n",
    "#        print (\"after down3\", y.shape)\n",
    "\n",
    "        # 16x64x32\n",
    "        if shortcut:\n",
    "            y = torch.cat((y, self.x2), 1)\n",
    "            y = F.relu(self.uconv2(y))\n",
    "        y = self.downward_net2(y)\n",
    "#        print (\"after down2\", y.shape)\n",
    "\n",
    "        # 8x128x64\n",
    "        y = self.downward_net1(y)\n",
    "#        print (\"after down1\", y.shape)\n",
    " \n",
    "        # 1x256x128\n",
    "        return y\n",
    "\n",
    "# Res_model = ResDAE()\n",
    "Res_model = torch.load(root_dir + 'recover/SSIM-CONV/DAE_SSIM.pkl')\n",
    "# print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_ssim\n",
    "criterion = pytorch_ssim.SSIM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr) #, momentum = mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_record = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epo in range(epoch):\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs = data\n",
    "        inputs = Variable(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        top = model.upward(inputs + white(inputs))\n",
    "        \n",
    "        outputs = model.downward(top, shortcut = True)\n",
    "        inputs = inputs.view(bs, 1, 256, 128)\n",
    "        outputs = outputs.view(bs, 1, 256, 128)\n",
    "        #with open ( root_dir + 'recover/L1loss_FC/recover_pic_epo_' + str(epo), 'w') as f:\n",
    "        #    json.dump(outputs.tolist(), f)\n",
    "        \n",
    "        loss = - criterion(outputs, inputs)\n",
    "        ssim_value = - loss.data.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_record.append(loss.item())\n",
    "        plt.figure(figsize = (20, 10))\n",
    "        plt.plot(loss_record)\n",
    "        plt.xlabel('iterations')\n",
    "        plt.ylabel('loss')\n",
    "        plt.savefig(root_dir + 'recover/SSIM-CONV/DAE_loss.png')\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            inn = inputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/\" + str(epo) + \"_\" + str(i) + \".png\", inn)\n",
    "            \n",
    "            out = outputs[0].view(256, 128).detach().numpy() * 255\n",
    "            cv2.imwrite(\"/home/tk/Documents/recover/SSIM-CONV/\" + str(epo) + \"_\" + str(i) + \"_re.png\", out)\n",
    "            \n",
    "            \n",
    "#        every_loss.append(loss.item())\n",
    "#        del inputs, data\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print ('[%d, %5d] loss: %.3f' % (epo, i, loss.item()))\n",
    "#            print ('[%d, %5d] ssim: %.3f' % (epo, i, ssim_value))\n",
    "   \n",
    "    gc.collect()\n",
    "    plt.close(\"all\")\n",
    "\n",
    "#    epoch_loss.append(np.mean(every_loss))\n",
    "#    every_loss = []\n",
    "    \n",
    "\n",
    "\n",
    "#        plt.figure(figsize = (20, 10))\n",
    "#        plt.plot(epoch_loss)\n",
    "#        plt.xlabel('epocs')\n",
    "#        plt.ylabel('epoch_loss')\n",
    "#        plt.savefig(root_dir + 'DAE_epoch_loss')\n",
    "\n",
    "    \n",
    "#=============================================\n",
    "#        Save Model & Loss\n",
    "#=============================================\n",
    "\n",
    "torch.save(model, root_dir + 'recover/SSIM-CONV/DAE_SSIM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_list = []\n",
    "mix_label_list = []\n",
    "class mixDataSet(Dataset):\n",
    "    def __init__(self, mix_dir, mix_label_dir):           \n",
    "        \n",
    "        with open(mix_dir + 'mix3.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(mix_label_dir + 'mix_label3.json') as f:\n",
    "            mix_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        mixlabel = torch.cat(mix_label_list, 0)\n",
    "        \n",
    "        self.mix_spec = mixblock\n",
    "        self.mix_label = mixlabel\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.mix_spec.shape[0]\n",
    "\n",
    "    def __getitem__(self, spec_index, label_index): \n",
    "\n",
    "        mix_spec = self.mix_spec[spec_index]\n",
    "        mix_label = self.mix_label[label_index]\n",
    "        return mix_spec, mix_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset = mixDataSet(mix_dir, mix_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixset.__getitem__(150,19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jh = open('/home/tk/Documents/mix/mix3.json').read()\n",
    "a = json.loads(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jh = open('/home/tk/Documents/mix_labels/mix_label3.json').read()\n",
    "b = json.loads(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(a)\n",
    "b = np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(b)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixfolder = os.listdir(mix_dir)\n",
    "mixfolder.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dir = root_dir + 'clean/' \n",
    "mix_dir = root_dir + 'mix/' \n",
    "clean_label_dir = root_dir + 'clean_labels/' \n",
    "mix_label_dir = root_dir + 'mix_labels/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1.0623e-07, 1.3231e-03, 5.5651e-06, 2.8495e-02, 1.3299e-04, 5.2328e-06, 7.2000e-04, 2.8051e-05, 3.9795e-02, 9.8838e-01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.max(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1.0578e-02, 3.3309e-01, 5.5529e-01, 1.0909e-02, 3.8428e-02, 1.3777e-03,\n",
    "         1.2060e-02, 4.5675e-02, 1.5102e-01, 5.5134e-01],\n",
    "        [4.0438e-05, 2.1043e-04, 1.2589e-01, 5.1516e-03, 6.8046e-04, 9.9534e-03,\n",
    "         1.8230e-03, 5.8381e-06, 2.6256e-01, 7.9211e-01],\n",
    "        [1.2880e-03, 2.0016e-02, 3.5143e-02, 4.8582e-02, 1.2939e-02, 1.1453e-02,\n",
    "         1.3557e-01, 3.3008e-02, 2.6488e-01, 2.6349e-01],\n",
    "        [3.3836e-03, 7.3263e-02, 1.7925e-02, 2.2843e-01, 4.1886e-02, 1.5798e-02,\n",
    "         2.4491e-02, 5.4132e-03, 1.6431e-01, 2.7407e-02],\n",
    "        [5.5855e-02, 1.1084e-01, 1.7244e-02, 6.8025e-04, 2.2809e-02, 3.1570e-01,\n",
    "         3.4346e-03, 3.9227e-01, 7.6018e-03, 1.7340e-01],\n",
    "        [2.1523e-01, 7.6109e-02, 4.9581e-02, 9.8011e-04, 1.0011e-01, 2.1144e-01,\n",
    "         1.9372e-02, 4.0609e-02, 1.3001e-02, 2.5672e-03],\n",
    "        [7.3727e-02, 2.0658e-01, 9.7838e-02, 5.0830e-04, 4.4293e-03, 4.8621e-03,\n",
    "         6.2715e-02, 4.8715e-02, 2.3443e-03, 8.7680e-04],\n",
    "        [1.0301e-04, 2.3858e-03, 1.3977e-02, 6.8922e-04, 8.5931e-03, 2.2245e-04,\n",
    "         9.8420e-01, 3.0137e-03, 6.7784e-02, 6.7894e-03],\n",
    "        [2.2825e-03, 1.6498e-01, 2.2518e-02, 1.0465e-03, 7.4058e-03, 8.8901e-01,\n",
    "         8.0338e-02, 1.3140e-01, 1.8809e-03, 3.2917e-03],\n",
    "        [8.2398e-03, 5.5224e-02, 2.2561e-02, 1.9494e-04, 2.3196e-02, 3.1043e-03,\n",
    "         3.8204e-01, 9.6084e-03, 1.9456e-03, 1.7574e-03],\n",
    "        [1.0623e-07, 1.3231e-03, 5.5651e-06, 2.8495e-02, 1.3299e-04, 5.2328e-06,\n",
    "         7.2000e-04, 2.8051e-05, 3.9795e-02, 9.8838e-01],\n",
    "        [3.3474e-03, 1.8030e-03, 9.8147e-01, 8.5557e-05, 2.4013e-05, 1.0115e-02,\n",
    "         8.4762e-04, 1.3052e-03, 7.6764e-03, 1.2625e-04],\n",
    "        [2.0284e-03, 6.2676e-04, 1.0758e-04, 1.1942e-06, 1.7032e-05, 2.7349e-04,\n",
    "         5.7749e-03, 2.9111e-01, 6.8634e-04, 1.4915e-04],\n",
    "        [1.3000e-03, 2.7204e-05, 2.6371e-03, 4.6776e-04, 1.0860e-05, 8.1055e-04,\n",
    "         3.0986e-04, 2.8704e-06, 9.5868e-02, 1.2196e-01],\n",
    "        [8.1448e-04, 5.3910e-06, 4.3349e-05, 9.9108e-01, 2.1359e-05, 2.3141e-08,\n",
    "         3.6492e-02, 8.2024e-09, 1.9351e-01, 6.2107e-05],\n",
    "        [4.6230e-05, 3.4710e-02, 2.7775e-03, 1.2075e-05, 1.6022e-04, 9.7403e-01,\n",
    "         1.1194e-02, 5.7854e-02, 4.9520e-06, 6.2279e-06]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predict = torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = []\n",
    "for i in predict:\n",
    "    b = np.zeros(10)\n",
    "    b[i] = 1\n",
    "    blank.append(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = torch.tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels == predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureDataSet(Dataset):\n",
    "    def __init__(self, clean_dir, clean_label_dir):\n",
    "                \n",
    "        for i in cleanfolder:\n",
    "            with open(clean_dir + '{}'.format(i)) as f:\n",
    "                clean_list.append(torch.Tensor(json.load(f)))\n",
    "                \n",
    "        for i in cleanlabelfolder:\n",
    "            with open(clean_label_dir + '{}'.format(i)) as f:\n",
    "                clean_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        cleanblock = torch.cat(clean_list, 0)\n",
    "        self.spec = torch.cat([cleanblock], 0)\n",
    "                \n",
    "        cleanlabel = torch.cat(clean_label_list, 0)\n",
    "        self.label = torch.cat([cleanlabel], 0)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.spec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        spec = self.spec[index]\n",
    "        label = self.label[index]\n",
    "        return spec, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/tk/Documents/'\n",
    "clean_dir = root_dir + 'clean/' \n",
    "mix_dir = root_dir + 'mix/' \n",
    "clean_label_dir = root_dir + 'clean_labels/' \n",
    "mix_label_dir = root_dir + 'mix_labels/' \n",
    "\n",
    "cleanfolder = os.listdir(clean_dir)\n",
    "cleanfolder.sort()\n",
    "\n",
    "cleanlabelfolder = os.listdir(clean_label_dir)\n",
    "cleanlabelfolder.sort()\n",
    "\n",
    "clean_list = []\n",
    "clean_label_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = featureDataSet(clean_dir, clean_label_dir)\n",
    "spec, label = c.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (spec, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### White noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def white_noise(num_samples):\n",
    "    mean = -10\n",
    "    std = 1\n",
    "    num_samples = num_samples\n",
    "    samples = np.random.normal(mean, std, size = num_samples)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pink Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voss(nrows, ncols = 16):\n",
    "    \n",
    "    import pandas as pd\n",
    "    array = np.empty((nrows, ncols))\n",
    "    array.fill(np.nan)\n",
    "    array[0, :] = np.random.random(ncols)\n",
    "    array[:, 0] = np.random.random(nrows)\n",
    "    \n",
    "    n = nrows\n",
    "    cols = np.random.geometric(0.5, n)\n",
    "    cols[cols >= ncols] = 0\n",
    "    \n",
    "    rows = np.random.randint(nrows, size = n)\n",
    "    array[rows, cols] = np.random.random(n)\n",
    "    \n",
    "    df = pd.DataFrame(array)\n",
    "    df.fillna(method = 'ffill', axis = 0, inplace = True)\n",
    "    total = df.sum(axis = 1)\n",
    "    \n",
    "    return total.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = voss(1000, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import colorednoise\n",
    "\n",
    "beta = 1\n",
    "samples = 1000\n",
    "y = colorednoise.powerlaw_psd_gaussian(beta, samples)\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y).reshape(100, 10)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = open('/home/tk/Documents/clean/clean10.json').read()\n",
    "a = json.loads(j)\n",
    "a = np.array(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]\n",
    "cv2.imwrite('/home/tk/Desktop/original.png', a[1]*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def white_noise(num_samples):\n",
    "    mean = 0\n",
    "    std = 0.3\n",
    "    num_samples = num_samples\n",
    "    samples = np.random.normal(mean, std, size = num_samples)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = white_noise(256*128)\n",
    "white = white.reshape(256, 128)\n",
    "\n",
    "a_white = a[1] + white\n",
    "cv2.imwrite('/home/tk/Desktop/original_white.png', a_white * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_white = a[1] + white\n",
    "cv2.imwrite('/home/tk/Desktop/original_white.png', a_white * 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_it(filename, input_path, output_path, length):\n",
    "    from pydub import AudioSegment\n",
    "    from pydub.utils import make_chunks\n",
    "    \n",
    "\n",
    "    # slice the file\n",
    "    myaudio = AudioSegment.from_file(input_path + filename, 'wav') \n",
    "    chunks = make_chunks(myaudio, length) # Make chunks\n",
    "\n",
    "    #Export all individual chunks as wav files\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_name = filename[:-4] + \"_{0}.wav\".format(i) # select first 6 characters as file name\n",
    "        print (\"exporting\", chunk_name)\n",
    "        chunk.export(output_path + chunk_name, format=\"wav\")\n",
    "\n",
    "    # dump the last slice (might be an incomplete slice)\n",
    "    dump_file = [] \n",
    "    chunk_list = os.listdir(output_path)\n",
    "    if '.DS_Store' in chunk_list:\n",
    "        chunk_list.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sliced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_audio = ['birdstudybook_', 'captaincook_', 'cloudstudies_02_clayden_12_', \n",
    "              'constructivebeekeeping_',\n",
    "              'discoursesbiologicalgeological_16_huxley_12_', \n",
    "              'natureguide_', 'pioneersoftheoldsouth_', \n",
    "              'pioneerworkalps_02_harper_12_', \n",
    "              'romancecommonplace_', 'travelstoriesretold_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sliced_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_list[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "sliced_list = []\n",
    "for name in full_audio:\n",
    "    for i in range(60, 160):\n",
    "        m = name + str(i) + '.wav'\n",
    "        sliced_list.append(m)\n",
    "        del m\n",
    "        cnt +=1\n",
    "        \n",
    "print (sliced_list)\n",
    "print (cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for j in sliced_list:\n",
    "    slice_it(j, '/home/tk/Documents/slice_10sec/', '/home/tk/Documents/sliced_pool/', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = os.listdir('/home/tk/Documents/slice_pointsec/')\n",
    "\n",
    "for ev in p:\n",
    "    os.remove('/home/tk/Documents/slice_pointsec/' + ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = os.listdir('/home/tk/Documents/sliced_pool/')\n",
    "print (len(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_audio = np.random.choice(g, size = 2, replace = False)\n",
    "if selected_audio[0][:8] == selected_audio[1][:8]:\n",
    "    selected_audio = np.random.choice(g, size = 2, replace = False)\n",
    "if r[0][:8] == r[1][:8]:\n",
    "    selected_audio = np.random.choice(g, size = 2, replace = False)\n",
    "print (selected_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.imwrite('/home/tk/Desktop/spec0.png', spec0 * 255)\n",
    "cv2.imwrite('/home/tk/Desktop/spec1.png', spec1 * 255)\n",
    "cv2.imwrite('/home/tk/Desktop/mixed_spec.png', mixed_spec * 255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/tk/Documents/'\n",
    "\n",
    "stock = []\n",
    "spec0_cluster = []\n",
    "spec1_cluster = []\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(100):\n",
    "        from data_process import gen_spectrogram\n",
    "\n",
    "        selected_audio = np.random.choice(g, size = 2, replace = False)\n",
    "\n",
    "        # prevent to select from the same source\n",
    "        if selected_audio[0][:8] == selected_audio[1][:8]: \n",
    "            selected_audio = np.random.choice(g, size = 2, replace = False)\n",
    "        if r[0][:8] == r[1][:8]:\n",
    "            selected_audio = np.random.choice(g, size = 2, replace = False)\n",
    "        print (selected_audio)\n",
    "\n",
    "        # generate 2 spectrograms & mix\n",
    "        gen_path0 = '/home/tk/Documents/sliced_pool/' + selected_audio[0]\n",
    "        gen_path1 = '/home/tk/Documents/sliced_pool/' + selected_audio[1]\n",
    "        spec0 = gen_spectrogram(gen_path0)\n",
    "        spec1 = gen_spectrogram(gen_path1)\n",
    "        mixed_spec = spec0 + spec1\n",
    "\n",
    "        # append the mixed spectrograms\n",
    "        stock.append(mixed_spec)\n",
    "        spec0_cluster.append(spec0)\n",
    "        spec1_cluster.append(spec1)\n",
    "\n",
    "    stock = np.array(stock)\n",
    "    print ('mixed_spec shape = ', stock.shape)\n",
    "\n",
    "    spec0_cluster = np.array(spec0_cluster)\n",
    "    print ('spec0 cluster shape = ', spec0_cluster.shape)\n",
    "\n",
    "    spec1_cluster = np.array(spec1_cluster)\n",
    "    print ('spec1 cluster shape = ', spec1_cluster.shape)\n",
    "\n",
    "    with open(root_dir + \"mix_pool/mix_spec/\" + str(j) + '.json', 'w') as jh:\n",
    "        json.dump(stock.tolist(), jh)\n",
    "\n",
    "    with open(root_dir + \"mix_pool/spec0/\" + str(j) + '.json', 'w') as jh:\n",
    "        json.dump(spec0_cluster.tolist(), jh)\n",
    "\n",
    "    with open(root_dir + \"mix_pool/spec1/\" + str(j) + '.json', 'w') as jh:\n",
    "        json.dump(spec1_cluster.tolist(), jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = np.array(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import json\n",
    "import data_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_audio = ['birdstudybook_', 'captaincook_', 'cloudstudies_02_clayden_12_', \n",
    "              'constructivebeekeeping_',\n",
    "              'discoursesbiologicalgeological_16_huxley_12_', \n",
    "              'natureguide_', 'pioneersoftheoldsouth_', \n",
    "              'pioneerworkalps_02_harper_12_', \n",
    "              'romancecommonplace_', 'travelstoriesretold_']\n",
    "\n",
    "folder_name = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "\n",
    "seq = enumerate(full_audio, folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "import os\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import json\n",
    "import data_process\n",
    "\n",
    "# parameters:\n",
    "\n",
    "## 10 sec slicing:\n",
    "full_audio_path = '/home/tk/Documents/full_audio/' # full audio will be stored here\n",
    "sec10_sliced_path = '/home/tk/Documents/slice_10sec/' # 10 sec sliced will be stored here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]\n",
    "a = np.mean(a)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(200, 200)\n",
    "b = np.random.randn(200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.zeros((200,200))\n",
    "c = c + a+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from data_process import gen_spectrogram\n",
    "import json\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "sliced_pool_path = '/home/tk/Documents/sliced_pool/'\n",
    "mixed_pool_path =  '/home/tk/Documents/mix_pool/'\n",
    "\n",
    "full_audio = ['birdstudybook', 'captaincook', 'cloudstudies_02_clayden_12', \n",
    "              'constructivebeekeeping',\n",
    "              'discoursesbiologicalgeological_16_huxley_12', \n",
    "              'natureguide', 'pioneersoftheoldsouth', \n",
    "              'pioneerworkalps_02_harper_12', \n",
    "              'romancecommonplace', 'travelstoriesretold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "blocks = 10\n",
    "for i in range(blocks):\n",
    "    for name in full_audio:\n",
    "        \n",
    "        all_clean_spec = []\n",
    "        if (mixed_pool_path + 'feature/' + name) == False:\n",
    "            os.mkdir(mixed_pool_path + 'feature/' + name)\n",
    "        \n",
    "        file_name_list = os.listdir(sliced_pool_path + name + '/clean/')\n",
    "        file_name = np.random.choice(file_name_list, 100)\n",
    "        \n",
    "        for k in file_name:\n",
    "            spec = gen_spectrogram(sliced_pool_path + name + '/clean/' + k)\n",
    "            print (k)\n",
    "            all_clean_spec.append(spec)\n",
    "            \n",
    "            \n",
    "        all_clean_spec = np.array(all_clean_spec)\n",
    "        all_clean_spec = np.stack(all_clean_spec)\n",
    "            \n",
    "        print (\"name = \", name , \", shape = \", all_clean_spec.shape)\n",
    "       \n",
    "    with open(mixed_pool_path +  '/feature/' + name + '/' + str(i) + '.json', 'w') as jh:\n",
    "        json.dump(all_clean_spec.tolist(), jh)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "import torch.nn.init as init\n",
    "import pytorch_ssim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import gc\n",
    "import cv2\n",
    "\n",
    "server = False\n",
    "\n",
    "root_dir = '/home/tk/Documents/'\n",
    "if server == True:\n",
    "    root_dir = '/home/guotingyou/cocktail_phase2/'\n",
    "\n",
    "\n",
    "clean_dir = '/home/tk/Documents/mix_pool/feature/' \n",
    "mix_dir = '/home/tk/Documents/mix_pool/mix_spec/' \n",
    "# clean_label_dir = '/home/tk/Documents/clean_labels/' \n",
    "mix_label_dir = '/home/tk/Documents/mix_pool/mix_labels/' \n",
    "target_spec_dir = '/home/tk/Documents/mix_pool/target_spec/'\n",
    "target_label_dir = '/home/tk/Documents/mix_pool/target_label/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, mix_dir, target_spec_dir, target_label_dir):           \n",
    "        \n",
    "        mix_list = []\n",
    "        target_spec_list = []\n",
    "        target_label_list = []\n",
    "\n",
    "        with open(mix_dir + 'mix_spec2.json') as f:\n",
    "            mix_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        with open(target_spec_dir + 'target_spec2.json') as f:\n",
    "            target_spec_list.append(torch.Tensor(json.load(f)))\n",
    "            \n",
    "        with open(target_label_dir + 'target_label2.json') as f:\n",
    "            target_label_list.append(torch.Tensor(json.load(f)))\n",
    "        \n",
    "        \n",
    "\n",
    "        mixblock = torch.cat(mix_list, 0)\n",
    "        targetspec = torch.cat(target_spec_list, 0)\n",
    "        targetlabel = torch.cat(target_label_list, 0)\n",
    "        \n",
    "        self.mix_spec = mixblock\n",
    "        self.target_spec = targetspec\n",
    "        self.target_label = targetlabel\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.mix_spec.shape[0]\n",
    "\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "\n",
    "        mix_spec = self.mix_spec[index]\n",
    "        target_spec = self.target_spec[index]\n",
    "        target_label = self.target_label[index]\n",
    "\n",
    "        return mix_spec, target_spec, target_label\n",
    "\n",
    "\n",
    "class featureDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, clean_dir, label):\n",
    "        \n",
    "        full_audio = ['birdstudybook', 'captaincook', 'cloudstudies_02_clayden_12', \n",
    "              'constructivebeekeeping',\n",
    "              'discoursesbiologicalgeological_16_huxley_12', \n",
    "              'natureguide', 'pioneersoftheoldsouth', \n",
    "              'pioneerworkalps_02_harper_12', \n",
    "              'romancecommonplace', 'travelstoriesretold']\n",
    "\n",
    "        feature_list = []\n",
    "\n",
    "        with open(clean_dir + full_audio[label] + '/0.json') as f:\n",
    "            featureblock = torch.Tensor(json.load(f))     \n",
    "        \n",
    "#        featureblock = torch.cat(feature_list, 0)\n",
    "        \n",
    "        self.featurespec = featureblock\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.featurespec.shape[0]\n",
    "\n",
    "                \n",
    "    def __getitem__(self): \n",
    "        \n",
    "        featurespec = self.featurespec\n",
    "        return featurespec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 128])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(clean_dir + 'birdstudybook/0.json') as f:\n",
    "    featureblock = torch.Tensor(json.load(f))     \n",
    "\n",
    "featureblock.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 128])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.mean(featureblock, 0, True)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================\n",
    "#        Define Dataloader\n",
    "#=============================================\n",
    "\n",
    "\n",
    "mixset = mixDataSet(mix_dir, target_spec_dir, target_label_dir)\n",
    "\n",
    "mixloader = torch.utils.data.DataLoader(dataset = mixset,\n",
    "                                        batch_size = bs,\n",
    "                                        shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, t, l = mixset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor([1., 6., 5., 5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset = featureDataSet(clean_dir, 2)\n",
    "a = featureset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 128])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
